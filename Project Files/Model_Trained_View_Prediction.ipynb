{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10031c-3650-4d6f-bd27-3fa0172c0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to your Excel file containing the dataset\n",
    "excel_file = 'ted_data_with_comments.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Create a new column for speaker names\n",
    "df['Speaker Names'] = ''\n",
    "\n",
    "# Assuming 'speakers' is the column containing speaker names in JSON format\n",
    "# Adjust the column name if it's different in your dataset\n",
    "for index, row in df.iterrows():\n",
    "    speaker_names = []\n",
    "    try:\n",
    "        speakers_info = json.loads(row['speakers'])\n",
    "        for speaker_info in speakers_info:\n",
    "            speaker_names.append(speaker_info['name'])\n",
    "    except ValueError:\n",
    "        pass  # Skip rows where JSON parsing fails\n",
    "    df.at[index, 'Speaker Names'] = ', '.join(speaker_names)\n",
    "\n",
    "# Write the updated DataFrame back to the Excel file\n",
    "df.to_excel(excel_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1cd381-f3c4-4c84-9ce3-9ede51075c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73569a5-328d-488d-ba22-4e5f7a745a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from a CSV file\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Print the column names to ensure we have the correct column\n",
    "print(df.columns)\n",
    "\n",
    "# Rename the column \"Unnamed: 17\" to \"comments\"\n",
    "df = df.rename(columns={'Unnamed: 17': 'comments'})\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('your_modified_file.csv', index=False)\n",
    "\n",
    "# Print the column names to verify the change\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23433768-c5f4-4fac-9d05-425060dc53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Datatime library for Date columns\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "\n",
    "# for remove Multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# For build pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor,StackingRegressor\n",
    "\n",
    "\n",
    "# for plot decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "# Model selection libraries\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# importing XGB regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Metrics libraries for model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Warnings module handles warnings in Python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b93d0-5aa2-46df-af42-4748dd91ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aeb16d-6cfc-423e-a259-e9f09fc8bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf1e87-f31f-4d9f-b78d-15dce95405fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c98d-e407-40df-97f4-0d99b073a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefe035-8cec-4828-a348-85340b76be14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(percentiles=[.25,.50,.75,.80,.85,.90,.95,.96,.97,.98,.99])\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c24e8-250c-4e29-9264-beef508fec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['comments']==0.0].sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7ba3c-8ff2-4479-a757-b00fe37df99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "  print(f'The unique values in {i} are {df[i].nunique()}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f15c4c-12a5-49fc-8ab9-ee27f10ab978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.astype({'_id': 'int32', 'views':'int32','comments':'int32', 'duration':'int32'})\n",
    "\n",
    "df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "\n",
    "df['recorded_date'] = pd.to_datetime(df['recorded_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc1b99-6e11-4a17-932e-1ee7b9376ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['_id', 'page_url', 'speakers','Unnamed: 16' ,'summary'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9922f9f-cc1d-463c-920a-193f81d97d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names to identify the exact column name\n",
    "print(df.columns)\n",
    "\n",
    "# Drop the column that matches the pattern '0'\n",
    "columns_to_drop = [col for col in df.columns if col.strip() == '0']\n",
    "\n",
    "# Drop the identified column(s)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943198e9-9409-497f-870f-60ea48c7ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d69fc-47eb-4bde-bf60-e038093959e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['views'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb15c7-a437-4c7f-a219-7ab1090435cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['views'] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad09b12-c7fc-4086-93e9-6ca433d94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([ 'related_videos', 'youtube_video_code','subtitle_languages','transcript'], axis=1, inplace=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194c774-1805-489f-b8c0-7c59a7bf6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a920a39-eef5-4525-aade-77e824f82299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb553cc7-be7d-482a-8284-b1a7ed8dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from a CSV file\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Print the column names to ensure we have the correct column\n",
    "print(df.columns)\n",
    "\n",
    "# Rename the column \"Unnamed: 17\" to \"comments\"\n",
    "df = df.rename(columns={'Speaker Names': 'speaker'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e57fe3-76d0-488c-b639-b8c255e73147",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_talks = df[['title', 'speaker', 'views']].sort_values('views', ascending=False)[0:15]\n",
    "popular_talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7cb0ea-1edf-4d0d-946f-c8fbe9571bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top15_views = df.groupby('speaker').views.sum().nlargest(15)\n",
    "top15_views = top15_views.reset_index()\n",
    "top15_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d84263-4b01-4bae-b2da-02e83e3d5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with top 15 speakers by comments\n",
    "top15_comments = df.groupby('speaker').comments.sum().nlargest(15)\n",
    "top15_comments = top15_comments.reset_index()\n",
    "top15_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762babc-6446-4238-af50-cb1ebdf26f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 1 visualization code\n",
    "# create the figure and subplots\n",
    "fig, axs = plt.subplots(2,1, figsize=(18,12))\n",
    "\n",
    "# create a barplot with top 15 speakers by views\n",
    "sns.barplot(x='views', y='speaker', data=top15_views, ax=axs[0])\n",
    "axs[0].set_title('Top 15 Speakers by Views')\n",
    "\n",
    "# create a barplot with top 15 speakers by comments\n",
    "sns.barplot(x='comments', y='speaker', data=top15_comments, ax=axs[1])\n",
    "axs[1].set_title('Top 15 Speakers by Comments')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b8049-29ce-45ee-b180-1f49f94cb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 2 visualization code\n",
    "#checking corr. with views column\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='comments', y='views', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792fa97-1592-4fad-ab92-7dc0801c738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 3 visualization code\n",
    "# checking distribution of comments column\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df['comments'], color='Red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4ad16-310e-4a38-b63f-8e2775d9fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(df[df['comments'] > 1100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027b587-c260-4b21-bcb3-68a744e3896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dropping indexes where comments are greater than 1100\n",
    "df.drop(df[df['comments']>1100].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86889bc7-6008-4821-a296-0a8a5664e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d49fee-33fd-4ddb-baf2-c3d670c72d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['comments']= df['comments'].replace(0, np.nan)\n",
    "df[\"comments\"].fillna(df[\"comments\"].median(), axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ad7e5-2616-45a7-8f9e-829ad543372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribution of comments column\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df['comments'], color='Red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aef7ad-21a0-436c-a680-609bf4f7372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 4 visualization code\n",
    "# check distribution of views column\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df['views'], color ='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23e8e4-6552-43c8-b731-e0f9be21342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 5 visualization code\n",
    "# check distribution of duration column\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df['duration'], color ='Orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342dd10-d565-47de-ae65-c36616aa0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change duration in sec. to min.\n",
    "\n",
    "df['duration'] = df['duration'] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3c4ed-a7cf-4160-905c-55b0a157a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'speaker_popularity' in the main DataFrame and assign the categories\n",
    "\n",
    "df['speaker_popularity'] = \"\"\n",
    "df.loc[df['views'] <= 500000, 'speaker_popularity'] = 'not_popular'\n",
    "df.loc[(df['views'] > 500000) & (df['views'] <= 1500000), 'speaker_popularity'] = 'avg_popular'\n",
    "df.loc[(df['views'] > 1500000) & (df['views'] <= 2500000), 'speaker_popularity'] = 'popular'\n",
    "df.loc[(df['views'] > 2500000) & (df['views'] <= 3500000), 'speaker_popularity'] = 'high_popular'\n",
    "df.loc[df['views'] > 3500000, 'speaker_popularity'] = 'extreme_popular'\n",
    "\n",
    "# check the dataset\n",
    "\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db55ad-b60e-446e-9cec-9e8661a85789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart - 6 visualization code\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.barplot(data=df, x='speaker_popularity', y='comments',\n",
    "            order=['not_popular', 'avg_popular', 'popular', 'high_popular', 'extreme_popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357bd30-2028-4473-9dc1-5d076c4f1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'video_rating' in the main DataFrame and assign the categories\n",
    "\n",
    "df['video_rating'] = \"\"\n",
    "df.loc[df['comments'] <= 50, 'video_rating'] = 1\n",
    "df.loc[(df['comments'] > 50) & (df['comments'] <= 120), 'video_rating'] = 2\n",
    "df.loc[(df['comments'] > 120) & (df['comments'] <= 200), 'video_rating'] = 3\n",
    "df.loc[(df['comments'] > 200) & (df['comments'] <= 300), 'video_rating'] = 4\n",
    "df.loc[df['comments'] > 300, 'video_rating'] = 5\n",
    "\n",
    "# check the dataset\n",
    "df.sample(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162adb0-6825-48ae-bb34-7b2de73173ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d342-3dc3-49c8-a072-c0207e8dedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'published_date' is in datetime format\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "# Check if the conversion was successful\n",
    "print(df['published_date'].head())\n",
    "\n",
    "# Create new columns for year, month, day name, and day number\n",
    "df['published_year'] = df['published_date'].dt.year\n",
    "df['published_month'] = df['published_date'].dt.month\n",
    "df['published_day'] = df['published_date'].dt.day_name()\n",
    "\n",
    "# Storing weekdays in order of numbers from 0 to 6 value\n",
    "daydict = {'Sunday': 0, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6}\n",
    "\n",
    "# Making new column holding information of day number\n",
    "df['published_daynumber'] = df['published_day'].map(daydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192a25d-27d6-4316-aa86-4e874bd925e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more column published_months_ago\n",
    "\n",
    "df['published_months_ago'] = ((2023 - df['published_year'])*12 + df['published_month'])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651da41d-09bb-4031-a9cf-f9bdaa2b3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe0906-9e21-4a87-9e31-6fc952e5fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['event'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b746a-94d7-4724-b7d8-fc68b67d962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column of each TED event type using existing column event\n",
    "\n",
    "ted_categories = ['TED-Ed','TEDx', 'TED', 'TEDGlobal', 'TEDSummit', 'TEDWomen', 'TED Residency']\n",
    "\n",
    "\n",
    "df['TEDevent_type'] = df['event'].map(lambda x: \"TEDx\" if x[0:4] == \"TEDx\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED-Ed\" if x[0:4] == \"TED_Ed\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED\" if x[0:4] == \"TED2\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDGlobal\" if x[0:4] == \"TEDG\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDWomen\" if x[0:4] == \"TEDW\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDSummit\" if x[0:4] == \"TEDS\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED Residency\" if x[0:13] == \"TED Residency\" else x)\n",
    "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"Other TED\" if x not in ted_categories else x)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e845ef-99f8-4713-aa68-6f6ae24ffe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# check the all events talkshows counts\n",
    "\n",
    "pd.DataFrame(df['TEDevent_type'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51943a6c-fffa-4bde-8fe5-28634969b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "\n",
    "# use duplicate dataframe for topics analysis\n",
    "dff = df.copy()\n",
    "\n",
    "dff['topics'] = dff['topics'].apply(lambda x: ast.literal_eval(x))\n",
    "s = dff.apply(lambda x: pd.Series(x['topics']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'topic'\n",
    "\n",
    "dff = dff.drop('topics', axis=1).join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfd265-8212-4616-8681-480b65e25bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'dff' DataFrame is already created and contains a 'topic' column\n",
    "# Example: dff = pd.read_csv('tedx_data.csv')\n",
    "\n",
    "# Convert 'topic' column to strings\n",
    "dff['topic'] = dff['topic'].astype(str)\n",
    "\n",
    "# Step 1: Count the occurrences of each topic\n",
    "pop_topic = pd.DataFrame(dff['topic'].value_counts()).reset_index()\n",
    "pop_topic.columns = ['topic', 'TEDtalks']\n",
    "\n",
    "# Step 2: Plot the bar chart\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='topic', y='TEDtalks', data=pop_topic.head(12))\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.title('Top 12 Popular Topics on TEDx Website')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of TED Talks')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea6f52-0b37-4d06-86fb-5051c4f63e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels = [\"speaker\", \"title\", \"recorded_date\", \"published_date\", \"event\",\"topics\"],axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66449e6d-7188-49f7-9d41-63891d8dcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df['likes'] = df['likes'].replace({'K': '*1e3', 'M': '*1e6'}, regex=True).map(pd.eval).astype(int)\n",
    "\n",
    "# If you're sure that only 'K' and 'M' are used for thousands and millions:\n",
    "# df['likes'] = df['likes'].str.replace('K', 'e3').str.replace('M', 'e6').astype(float)\n",
    "\n",
    "print(df.head())  # Check the first few rows to confirm the conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7051ed4-837d-4bc1-9664-7b51912add22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again change data-types of columns\n",
    "\n",
    "df = df.astype({'comments':'int64', 'views':'int64','video_rating':'int64'})\n",
    "\n",
    "df = df.astype({\n",
    "    'speaker_popularity': 'category',\n",
    "    'published_day': 'category',\n",
    "    'TEDevent_type': 'category'\n",
    "})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21445626-c71e-4fc0-ba61-0cddbc2d36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# create a new DataFrame with only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'int32', 'float32', 'float64']).drop(['views'], axis=1)\n",
    "\n",
    "# calculate VIF for each column\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(numeric_cols.values, i) for i in range(numeric_cols.shape[1])]\n",
    "vif[\"features\"] = numeric_cols.columns\n",
    "\n",
    "# print the results\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf0d56-4b59-44c0-bba3-d5328616d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: drop published_months_ago column\n",
    "df.drop(['published_year','published_month', 'published_months_ago','video_rating'], axis=1, inplace=True)\n",
    "\n",
    "# Step 2: calculate VIF\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'int32', 'float32', 'float64']).drop(['views'], axis=1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(numeric_cols.values, i) for i in range(numeric_cols.shape[1])]\n",
    "vif[\"features\"] = numeric_cols.columns\n",
    "\n",
    "# print the results\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13462f-ff0a-4d5c-ae06-1bdfd886611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # use Yeo - Johnson Transform for views column and then we train test split the data\n",
    "\n",
    "# pt = PowerTransformer()\n",
    "# df['views'] = pt.fit_transform(pd.DataFrame(df['views']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a9bcd-00f1-4940-afc3-f47eae85c078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056deeda-75e0-4f0a-8c25-b3ebe54ad102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e06506-a82b-49c1-b45d-b85bd549149a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ea3f4-9603-4a80-a538-a5dfbbd1b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fbe58-1d1f-4107-848f-76fb7a910746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccbcb4-f257-4a56-b7b2-350e76f7c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ML Model - 1 Implementation\n",
    "# split the Dataset into independent(x) and dependent(y) Dataset\n",
    "\n",
    "X = df.drop(columns=['views'])\n",
    "y = df['views']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefab4ce-0bfe-41b0-b62b-e20d5135a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d055c-9a01-4b00-9939-b3aa02b05f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226afacb-36cd-41ca-99b2-ff7159f6292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling train_test_split() to get the training and testing data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# split sizes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043732c-52cf-4b49-b1ea-58c5b3421521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "# Assuming you already have a DataFrame named df\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "step1 = ColumnTransformer(transformers=[\n",
    "    ('col_tnf', StandardScaler(), [0, 1, 3, 5]),\n",
    "    ('col_tnf1', PowerTransformer(), [0, 1, 3]),\n",
    "    ('col_tnf2', OneHotEncoder(sparse_output=False, drop='first'), [4, 6]),\n",
    "    ('col_tnf3', OrdinalEncoder(categories=[['not_popular', 'avg_popular', 'popular', 'high_popular', 'extreme_popular']]), [2])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', step1)\n",
    "])\n",
    "\n",
    "# Display pipeline\n",
    "set_config(display='diagram')\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56aa736-aa01-492c-a4dc-da756990ad39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ccd22-0db1-4cd6-bb89-9971c3d5a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df['likes'] = df['likes'].replace({'K': '*1e3', 'M': '*1e6'}, regex=True).map(pd.eval).astype(int)\n",
    "\n",
    "# If you're sure that only 'K' and 'M' are used for thousands and millions:\n",
    "# df['likes'] = df['likes'].str.replace('K', 'e3').str.replace('M', 'e6').astype(float)\n",
    "\n",
    "print(df.head())  # Check the first few rows to confirm the conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9310c-2702-49a6-9b90-11bc30299341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the columns which are numeric and categorical\n",
    "numeric_features = ['duration', 'likes', 'comments', 'published_daynumber']  # Numeric features\n",
    "categorical_features = ['speaker_popularity', 'published_day', 'TEDevent_type']  # Categorical features\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = StandardScaler()  # StandardScaler for numeric features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # OneHotEncoder for categorical features\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training dataset\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict the train and test dataset\n",
    "y_pred_train = pipe.predict(X_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Display pipeline diagram\n",
    "display(pipe)\n",
    "\n",
    "# Linear Regression model output scores\n",
    "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_train, y_pred_train)) * ((X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_test, y_pred))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_test, y_pred)) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mThe performance metrics\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('MAE', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE', np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0502d9a-1ab8-41dd-9923-c4ff14d80563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_pred)\n",
    "plt.plot(np.array(y_test))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.xlabel('No. of Test Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02760c4-a5a6-4759-92b9-488fb43fc2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define the columns which are numeric and categorical\n",
    "numeric_features = ['duration', 'likes', 'comments', 'published_daynumber']  # Numeric features\n",
    "categorical_features = ['speaker_popularity', 'published_day', 'TEDevent_type']  # Categorical features\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = StandardScaler()  # StandardScaler for numeric features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # OneHotEncoder for categorical features\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the Ridge Regression model\n",
    "model = Ridge()\n",
    "\n",
    "# Define parameters for hyperparameter tuning\n",
    "parameters = {'alpha': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 3, 5, 8, 12, 15, 18, 21, 25]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "Reg_ridge = GridSearchCV(model, parameters, cv=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Reg_ridge)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training dataset\n",
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "# Predict the train and test dataset\n",
    "y_pred_train = pipe2.predict(X_train)\n",
    "y_pred = pipe2.predict(X_test)\n",
    "\n",
    "# Display pipeline diagram\n",
    "display(pipe2)\n",
    "\n",
    "# Ridge Regression model output scores\n",
    "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_train, y_pred_train)) * ((X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_test, y_pred))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_test, y_pred)) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mCross-validation score and best params\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print(\"The best parameters are\", Reg_ridge.best_params_)\n",
    "print('Cross-validation score', Reg_ridge.best_score_)\n",
    "print('\\n')\n",
    "print('\\033[1mThe performance metrics\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('MAE', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE', np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d15671-be9b-44fb-9aff-59daf4844100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_pred)\n",
    "plt.plot(np.array(y_test))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.xlabel('No. of Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643e51b-9939-4595-9a5a-6dc045b33fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5a81e-9484-4da7-8434-4dfcf516c83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0215e67-48d0-42cf-bb13-4fb66d80d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define the columns which are numeric and categorical\n",
    "numeric_features = ['duration', 'likes', 'comments', 'published_daynumber']  # Numeric features\n",
    "categorical_features = ['speaker_popularity', 'published_day', 'TEDevent_type']  # Categorical features\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = StandardScaler()  # StandardScaler for numeric features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # OneHotEncoder for categorical features\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the Lasso Regression model\n",
    "model = Lasso()\n",
    "\n",
    "# Define parameters for hyperparameter tuning\n",
    "parameters = {'alpha': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 3, 4, 5, 8, 12, 15, 18, 21, 25]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "Reg_Lasso = GridSearchCV(model, parameters, cv=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe3 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Reg_Lasso)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training dataset\n",
    "pipe3.fit(X_train, y_train)\n",
    "\n",
    "# Predict the train and test dataset\n",
    "y_pred_train = pipe3.predict(X_train)\n",
    "y_pred = pipe3.predict(X_test)\n",
    "\n",
    "# Display pipeline diagram\n",
    "display(pipe3)\n",
    "\n",
    "# Lasso Regression model output scores\n",
    "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_train, y_pred_train)) * ((X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_test, y_pred))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_test, y_pred)) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mCross-validation score and best params\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print(\"The best parameters are\", Reg_Lasso.best_params_)\n",
    "print('Cross-validation score', Reg_Lasso.best_score_)\n",
    "print('\\n')\n",
    "print('\\033[1mThe performance metrics\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('MAE', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE', np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510167d-a1a7-4b49-8b2b-4e196c4be462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_pred)\n",
    "plt.plot(np.array(y_test))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.xlabel('No. of Test Data')\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b9348-6c68-4492-b0a0-b624fcba50d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00644c11-1738-473b-9545-2073599ff1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c0295-67df-4cba-8b26-1850d0beb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define the columns which are numeric and categorical\n",
    "numeric_features = ['duration', 'likes', 'comments', 'published_daynumber']  # Numeric features\n",
    "categorical_features = ['speaker_popularity', 'published_day', 'TEDevent_type']  # Categorical features\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = StandardScaler()  # StandardScaler for numeric features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # OneHotEncoder for categorical features\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Define parameters for hyperparameter tuning\n",
    "parameters = {\n",
    "    'criterion': ['squared_error'],  # 'friedman_mse', 'absolute_error'\n",
    "    'splitter': ['best'],  # random\n",
    "    'max_depth': [6],  # 4,5,6,7,8,9,None\n",
    "    'max_features': [1.0]  # 0.25,0.50,0.75,0.85\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "dtr = GridSearchCV(model, param_grid=parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "step2 = dtr\n",
    "\n",
    "# Create the pipeline\n",
    "pipe4 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', step2)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training dataset\n",
    "pipe4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the train and test dataset\n",
    "y_pred_train = pipe4.predict(X_train)\n",
    "y_pred = pipe4.predict(X_test)\n",
    "\n",
    "# Display pipeline diagram\n",
    "display(pipe4)\n",
    "\n",
    "# Decision Tree Regressor model output scores\n",
    "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_train, y_pred_train)) * ((X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('R2 score', r2_score(y_test, y_pred))\n",
    "print('Adjusted R2 score', (1 - (1 - r2_score(y_test, y_pred)) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1))))\n",
    "print('\\n')\n",
    "print('\\033[1mCross-validation score and best params\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print(\"The best parameters is\", dtr.best_params_)\n",
    "print('Cross-validation score', dtr.best_score_)\n",
    "print('\\n')\n",
    "print('\\033[1mThe performance metrics\\033[0m')\n",
    "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
    "print('MAE', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE', np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923fade-3aef-4e17-b106-d25f6b2bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_pred)\n",
    "plt.plot(np.array(y_test))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.xlabel('No. of Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc3924-dbaa-460f-888c-34e694f44543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(pipe, 'model_pipeline.pkl')\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = joblib.load('model_pipeline.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006e654-39f5-4ae7-a29c-efe0b7312d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cacd4a-3939-4a3d-974a-4989d88ca0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d91f20-eff5-4e4f-a996-d5f8b80f9c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada1076-2654-4e3a-98ce-57baa510dbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab36ec8-08e6-464b-9b30-213f49697bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cf354-2090-4e32-be3e-d59dd35cb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import isodate\n",
    "from datetime import datetime\n",
    "\n",
    "# Your API key and video ID\n",
    "API_KEY = 'AIzaSyDGUBNSWC0ACrdpEu6VJKEgodZXYKv8WM0'\n",
    "video_id = input(\"Enter the video code\")\n",
    "\n",
    "# Fetch video details\n",
    "video_url = f'https://www.googleapis.com/youtube/v3/videos?id={video_id}&key={API_KEY}&part=snippet,statistics,contentDetails'\n",
    "video_response = requests.get(video_url)\n",
    "video_data = video_response.json()\n",
    "\n",
    "title = \"\"\n",
    "likes = 0\n",
    "views = 0\n",
    "comment_count = 0\n",
    "total_minutes = 0.0\n",
    "publish_day_number = 0\n",
    "\n",
    "day_mapping = {'Sunday': 0, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6}\n",
    "\n",
    "if 'items' in video_data and len(video_data['items']) > 0:\n",
    "    video_info = video_data['items'][0]\n",
    "    \n",
    "    # Extracting statistics and content details\n",
    "    title = video_info['snippet']['title']\n",
    "    likes = int(video_info['statistics']['likeCount'])\n",
    "    views = int(video_info['statistics']['viewCount'])\n",
    "    comment_count = int(video_info['statistics']['commentCount'])\n",
    "    duration_str = video_info['contentDetails']['duration']\n",
    "    \n",
    "    # Decode the duration\n",
    "    duration = isodate.parse_duration(duration_str)\n",
    "    total_minutes = duration.total_seconds() / 60.0\n",
    "    \n",
    "    # Extract and convert publish date to day number\n",
    "    publish_date_str = video_info['snippet']['publishedAt']\n",
    "    publish_date = datetime.fromisoformat(publish_date_str[:-1])  # Remove 'Z' and convert to datetime\n",
    "    publish_day_name = publish_date.strftime('%A')  # Get the day of the week\n",
    "    publish_day_number = day_mapping[publish_day_name]  # Map day name to number\n",
    "    \n",
    "    print(f'Title: {title}')\n",
    "    print(f'Likes: {likes}')\n",
    "    print(f'Views: {views}')\n",
    "    print(f'Comment Count: {comment_count}')\n",
    "    print(f'Duration: {total_minutes:.2f} minutes')\n",
    "    print(f'Published Day Number: {publish_day_number}')\n",
    "else:\n",
    "    print('Video not found or API quota exceeded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e6944-17f2-480c-9c52-1e58d7942789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a01d0-b8a0-4f2b-954f-5598dea3d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define the columns which are numeric\n",
    "numeric_features = ['duration', 'likes', 'comments', 'published_daynumber']\n",
    "\n",
    "# Define preprocessing steps for numeric features\n",
    "numeric_transformer = StandardScaler()  # StandardScaler for numeric features\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Define the Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Define parameters for hyperparameter tuning\n",
    "parameters = {\n",
    "    'criterion': ['squared_error'],  # Other options: 'friedman_mse', 'absolute_error'\n",
    "    'splitter': ['best'],  # Other option: 'random'\n",
    "    'max_depth': [6],  # Other options: 4,5,6,7,8,9,None\n",
    "    'max_features': [1.0]  # Other options: 0.25,0.50,0.75,0.85\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "dtr = GridSearchCV(model, param_grid=parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe4 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', dtr)\n",
    "])\n",
    "\n",
    "# Assuming X_train and y_train are defined\n",
    "# Fit the pipeline on training dataset\n",
    "pipe4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the train and test dataset\n",
    "y_pred_train = pipe4.predict(X_train)\n",
    "y_pred = pipe4.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'R2 Score (Train): {r2_score(y_train, y_pred_train)}')\n",
    "print(f'Mean Absolute Error (Train): {mean_absolute_error(y_train, y_pred_train)}')\n",
    "print(f'Mean Squared Error (Train): {mean_squared_error(y_train, y_pred_train)}')\n",
    "print(f'R2 Score (Test): {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean Absolute Error (Test): {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean Squared Error (Test): {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "# Get user input\n",
    "# Example inputs, replace these with actual values as needed\n",
    "duration = total_minutes  # e.g., 10.0\n",
    "video_likes = likes  # e.g., 100\n",
    "comments = comment_count  # e.g., 20\n",
    "published_daynumber = publish_day_number  # e.g., 2\n",
    "\n",
    "# Create a DataFrame for the input\n",
    "user_input = pd.DataFrame([[duration, video_likes, comments, published_daynumber]], columns=numeric_features)\n",
    "\n",
    "# Predict views based on user input\n",
    "predicted_views = pipe4.predict(user_input)\n",
    "\n",
    "print(f'Predicted views: {predicted_views[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d02aef-520b-4f76-a2ef-a8895d9e1650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce4045-e94c-4c8f-adb1-bed5d9577489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
