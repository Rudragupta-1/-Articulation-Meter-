{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python mediapipe numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize MediaPipe Holistics\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, 20, (output_width, output_height))\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "\n",
    "# Process each frame from the camera feed\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    left_hand_sum_distance = 0\n",
    "    if hand_results.left_hand_landmarks is not None:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    right_hand_sum_distance = 0\n",
    "    if hand_results.right_hand_landmarks is not None:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Update the frame index\n",
    "    frame_index += 1\n",
    "\n",
    "    # Filter out None values and calculate the min and max values for each graph\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    \n",
    "    min_left_hand = min(filtered_left_hand) if filtered_left_hand else 0\n",
    "    max_left_hand = max(filtered_left_hand) if filtered_left_hand else 0\n",
    "    min_right_hand = min(filtered_right_hand) if filtered_right_hand else 0\n",
    "    max_right_hand = max(filtered_right_hand) if filtered_right_hand else 0\n",
    "\n",
    "    # Calculate engagement level\n",
    "    engagement_level = [(lh + rh) / 2 if lh is not None and rh is not None else None for lh, rh in zip(left_hand, right_hand)]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "    \n",
    "    min_engagement = min(filtered_engagement_level) if filtered_engagement_level else 0\n",
    "    max_engagement = max(filtered_engagement_level) if filtered_engagement_level else 1\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    plt.figure(figsize=(output_width / 100, 3 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(np.arange(frame_index), left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "    plt.plot(np.arange(frame_index), right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Hand Movements', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Engagement Graph\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(np.arange(frame_index), engagement_level, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "    plt.ylim([min_engagement, max_engagement])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Engagement Level', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the graph with labels\n",
    "    plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.clf()\n",
    "\n",
    "    graph = cv2.imread('temp_graph.png')\n",
    "\n",
    "    # Resize the graph to match the video width and increased height\n",
    "    graph = cv2.resize(graph, (output_width, 3 * graph_height))\n",
    "\n",
    "    # Display the input video\n",
    "    cv2.imshow('Input Video', frame)\n",
    "\n",
    "    # Display the graph with labels\n",
    "    cv2.imshow('Graph', graph)\n",
    "\n",
    "    # Write the frame with graph to the output video\n",
    "    out.write(cv2.resize(frame, (output_width, output_height)))\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video objects and close OpenCV windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Pose and Holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles = []\n",
    "engagement_level = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, 20, (output_width, output_height))\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "\n",
    "def process_frame(frame, frame_index):\n",
    "    global left_hand, right_hand, shoulder_midpoints, head_turn_angles, engagement_level\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    # Draw landmarks and calculate distances for left and right hands\n",
    "    left_hand_sum_distance = 0\n",
    "    right_hand_sum_distance = 0\n",
    "\n",
    "    if hand_results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    if hand_results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        shoulder_midpoint = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint)\n",
    "\n",
    "        left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE]\n",
    "        right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "        # Calculate head turn angle\n",
    "        if left_eye and right_eye and nose:\n",
    "            eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "            nose_vector = np.array([nose.x - (left_eye.x + right_eye.x) / 2, nose.y - (left_eye.y + right_eye.y) / 2])\n",
    "            cosine_angle = np.dot(eye_line_vector, nose_vector) / (np.linalg.norm(eye_line_vector) * np.linalg.norm(nose_vector))\n",
    "            head_turn_angle = np.arccos(cosine_angle) * (180 / np.pi)\n",
    "            head_turn_angles.append(head_turn_angle)\n",
    "        else:\n",
    "            head_turn_angles.append(None)\n",
    "    else:\n",
    "        shoulder_midpoints.append(None)\n",
    "        head_turn_angles.append(None)\n",
    "\n",
    "    # Calculate engagement level\n",
    "    avg_hand_movement = (left_hand_sum_distance + right_hand_sum_distance) / 2 if left_hand_sum_distance and right_hand_sum_distance else None\n",
    "    avg_head_turn = np.mean(head_turn_angles[-5:]) if head_turn_angles[-5:] else None\n",
    "    if avg_hand_movement and avg_head_turn:\n",
    "        engagement = avg_hand_movement + avg_head_turn\n",
    "    else:\n",
    "        engagement = None\n",
    "    engagement_level.append(engagement)\n",
    "\n",
    "    # Filter out None values for graphs\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "    filtered_head_turn_angles = [point for point in head_turn_angles if point is not None]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "\n",
    "    # Calculate min and max values for y-axis limits\n",
    "    min_left_hand = min(filtered_left_hand) if filtered_left_hand else 0\n",
    "    max_left_hand = max(filtered_left_hand) if filtered_left_hand else 1\n",
    "    min_right_hand = min(filtered_right_hand) if filtered_right_hand else 0\n",
    "    max_right_hand = max(filtered_right_hand) if filtered_right_hand else 1\n",
    "    min_shoulder_midpoint = min(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 0\n",
    "    max_shoulder_midpoint = max(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 1\n",
    "    min_head_turn_angle = min(filtered_head_turn_angles) if filtered_head_turn_angles else 0\n",
    "    max_head_turn_angle = max(filtered_head_turn_angles) if filtered_head_turn_angles else 1\n",
    "    min_engagement = min(filtered_engagement_level) if filtered_engagement_level else 0\n",
    "    max_engagement = max(filtered_engagement_level) if filtered_engagement_level else 1\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    plt.figure(figsize=(output_width / 100, 4 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    plt.subplot(4, 1, 1)\n",
    "    if len(filtered_left_hand) > 0 and len(filtered_right_hand) > 0:\n",
    "        plt.plot(np.arange(len(filtered_left_hand)), filtered_left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "        plt.plot(np.arange(len(filtered_right_hand)), filtered_right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Hand Movements', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Shoulder Midpoints Graph\n",
    "    plt.subplot(4, 1, 2)\n",
    "    if len(filtered_shoulder_midpoints) > 0:\n",
    "        plt.plot(np.arange(len(filtered_shoulder_midpoints)), filtered_shoulder_midpoints, color='orange', linewidth=line_width, marker='o', markersize=1, label='Shoulder Midpoints')\n",
    "    plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Shoulder Midpoints', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Head Turn Angles Graph\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(filtered_head_turn_angles) > 0:\n",
    "        plt.plot(np.arange(len(filtered_head_turn_angles)), filtered_head_turn_angles, color='purple', linewidth=line_width, marker='o', markersize=1, label='Head Turn Angles')\n",
    "    plt.ylim([min_head_turn_angle, max_head_turn_angle])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Head Turn Angles', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Engagement Level Graph\n",
    "    plt.subplot(4, 1, 4)\n",
    "    if len(filtered_engagement_level) > 0:\n",
    "        plt.plot(np.arange(len(filtered_engagement_level)), filtered_engagement_level, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "    plt.ylim([min_engagement, max_engagement])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Engagement Level', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the graph with labels\n",
    "    plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.clf()\n",
    "\n",
    "    graph = cv2.imread('temp_graph.png')\n",
    "    graph = cv2.resize(graph, (output_width, 4 * graph_height))\n",
    "\n",
    "    return frame, graph\n",
    "\n",
    "def main():\n",
    "    global frame_index\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            future = executor.submit(process_frame, frame, frame_index)\n",
    "            processed_frame, graph = future.result()\n",
    "\n",
    "            # Update the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Display the input video\n",
    "            cv2.imshow('Input Video', processed_frame)\n",
    "\n",
    "            # Display the graph with labels\n",
    "            cv2.imshow('Graph', graph)\n",
    "\n",
    "            # Write the frame with graph to the output video\n",
    "            out.write(cv2.resize(processed_frame, (output_width, output_height)))\n",
    "\n",
    "            # Break the loop if the 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # Slow down the frame rate\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Release video objects and close OpenCV windows\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper codes are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Pose and Holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles = []\n",
    "engagement_level = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, 20, (output_width, output_height))\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "\n",
    "def calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles):\n",
    "    # Use recent history to determine engagement level\n",
    "    history_len = 10  # Number of frames to consider for engagement calculation\n",
    "\n",
    "    # Slicing the recent history\n",
    "    recent_left_hand = left_hand[-history_len:]\n",
    "    recent_right_hand = right_hand[-history_len:]\n",
    "    recent_shoulder_midpoints = shoulder_midpoints[-history_len:]\n",
    "    recent_head_turn_angles = head_turn_angles[-history_len:]\n",
    "\n",
    "    # Calculate average movements\n",
    "    avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
    "    avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
    "    avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
    "    avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
    "\n",
    "    # Engagement calculation based on movement thresholds\n",
    "    hand_threshold = 0.1  # Movement threshold for hands\n",
    "    shoulder_threshold = 0.05  # Movement threshold for shoulders\n",
    "    head_turn_threshold = 5.0  # Angle threshold for head turns (degrees)\n",
    "\n",
    "    hand_movement = (avg_left_hand is not None and avg_left_hand > hand_threshold) or (avg_right_hand is not None and avg_right_hand > hand_threshold)\n",
    "    shoulder_movement = avg_shoulder is not None and avg_shoulder > shoulder_threshold\n",
    "    head_movement = avg_head_turn is not None and avg_head_turn > head_turn_threshold\n",
    "\n",
    "    if hand_movement and shoulder_movement and head_movement:\n",
    "        return 'inspired'\n",
    "    elif hand_movement and (shoulder_movement or head_movement):\n",
    "        return 'interactive'\n",
    "    else:\n",
    "        return 'attentive'\n",
    "\n",
    "def process_frame(frame, frame_index):\n",
    "    global left_hand, right_hand, shoulder_midpoints, head_turn_angles, engagement_level\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    # Draw landmarks and calculate distances for left and right hands\n",
    "    left_hand_sum_distance = 0\n",
    "    right_hand_sum_distance = 0\n",
    "\n",
    "    if hand_results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    if hand_results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        shoulder_midpoint = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint)\n",
    "\n",
    "        left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE]\n",
    "        right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "        # Calculate head turn angle\n",
    "        if left_eye and right_eye and nose:\n",
    "            eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "            nose_vector = np.array([nose.x - (left_eye.x + right_eye.x) / 2, nose.y - (left_eye.y + right_eye.y) / 2])\n",
    "            cosine_angle = np.dot(eye_line_vector, nose_vector) / (np.linalg.norm(eye_line_vector) * np.linalg.norm(nose_vector))\n",
    "            head_turn_angle = np.arccos(cosine_angle) * (180 / np.pi)\n",
    "            head_turn_angles.append(head_turn_angle)\n",
    "        else:\n",
    "            head_turn_angles.append(None)\n",
    "    else:\n",
    "        shoulder_midpoints.append(None)\n",
    "        head_turn_angles.append(None)\n",
    "\n",
    "    # Calculate engagement level\n",
    "    engagement = calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles)\n",
    "    engagement_level.append(engagement)\n",
    "\n",
    "    # Filter out None values for graphs\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "    filtered_head_turn_angles = [point for point in head_turn_angles if point is not None]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "\n",
    "    # Calculate min and max values for y-axis limits\n",
    "    min_left_hand = min(filtered_left_hand) if filtered_left_hand else 0\n",
    "    max_left_hand = max(filtered_left_hand) if filtered_left_hand else 1\n",
    "    min_right_hand = min(filtered_right_hand) if filtered_right_hand else 0\n",
    "    max_right_hand = max(filtered_right_hand) if filtered_right_hand else 1\n",
    "    min_shoulder_midpoint = min(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 0\n",
    "    max_shoulder_midpoint = max(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 1\n",
    "    min_head_turn_angle = min(filtered_head_turn_angles) if filtered_head_turn_angles else 0\n",
    "    max_head_turn_angle = max(filtered_head_turn_angles) if filtered_head_turn_angles else 1\n",
    "    min_engagement = 1\n",
    "    max_engagement = 3\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    plt.figure(figsize=(output_width / 100, 4 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    plt.subplot(4, 1, 1)\n",
    "    if len(filtered_left_hand) > 0 and len(filtered_right_hand) > 0:\n",
    "        plt.plot(np.arange(len(filtered_left_hand)), filtered_left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "        plt.plot(np.arange(len(filtered_right_hand)), filtered_right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Hand Movements', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Shoulder Midpoints Graph\n",
    "    plt.subplot(4, 1, 2)\n",
    "    if len(filtered_shoulder_midpoints) > 0:\n",
    "        plt.plot(np.arange(len(filtered_shoulder_midpoints)), filtered_shoulder_midpoints, color='orange', linewidth=line_width, marker='o', markersize=1, label='Shoulder Midpoints')\n",
    "    plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Shoulder Midpoints', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Head Turn Angles Graph\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(filtered_head_turn_angles) > 0:\n",
    "        plt.plot(np.arange(len(filtered_head_turn_angles)), filtered_head_turn_angles, color='purple', linewidth=line_width, marker='o', markersize=1, label='Head Turn Angles')\n",
    "    plt.ylim([min_head_turn_angle, max_head_turn_angle])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Head Turn Angles', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Engagement Level Graph\n",
    "    engagement_colors = {'attentive': 'green', 'interactive': 'blue', 'inspired': 'red'}\n",
    "    engagement_numeric = [1 if e == 'attentive' else 2 if e == 'interactive' else 3 for e in filtered_engagement_level]\n",
    "    plt.subplot(4, 1, 4)\n",
    "    if len(filtered_engagement_level) > 0:\n",
    "        plt.plot(np.arange(len(filtered_engagement_level)), engagement_numeric, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "        for i, level in enumerate(filtered_engagement_level):\n",
    "            plt.plot(i, engagement_numeric[i], color=engagement_colors[level], marker='o')\n",
    "    plt.ylim([0.5, 3.5])\n",
    "    plt.yticks([1, 2, 3], ['Attentive', 'Interactive', 'Inspired'])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Engagement Level', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the graph with labels\n",
    "    plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.clf()\n",
    "\n",
    "    graph = cv2.imread('temp_graph.png')\n",
    "    graph = cv2.resize(graph, (output_width, 4 * graph_height))\n",
    "\n",
    "    return frame, graph\n",
    "\n",
    "def main():\n",
    "    global frame_index\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            future = executor.submit(process_frame, frame, frame_index)\n",
    "            processed_frame, graph = future.result()\n",
    "\n",
    "            # Update the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Display the input video\n",
    "            cv2.imshow('Input Video', processed_frame)\n",
    "\n",
    "            # Display the graph with labels\n",
    "            cv2.imshow('Graph', graph)\n",
    "\n",
    "            # Write the frame with graph to the output video\n",
    "            out.write(cv2.resize(processed_frame, (output_width, output_height)))\n",
    "\n",
    "            # Break the loop if the 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # Slow down the frame rate\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Release video objects and close OpenCV windows\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above is fine , some more refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Pose and Holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles = []\n",
    "engagement_level = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, 20, (output_width, output_height))\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "\n",
    "def calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles):\n",
    "    # Use recent history to determine engagement level\n",
    "    history_len = 10  # Number of frames to consider for engagement calculation\n",
    "\n",
    "    # Slicing the recent history\n",
    "    recent_left_hand = left_hand[-history_len:]\n",
    "    recent_right_hand = right_hand[-history_len:]\n",
    "    recent_shoulder_midpoints = shoulder_midpoints[-history_len:]\n",
    "    recent_head_turn_angles = head_turn_angles[-history_len:]\n",
    "\n",
    "    # Calculate average movements\n",
    "    avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
    "    avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
    "    avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
    "    avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
    "\n",
    "    # Engagement calculation based on movement thresholds\n",
    "    hand_threshold = 0.1  # Movement threshold for hands\n",
    "    shoulder_threshold = 0.1  # Increased movement threshold for shoulders\n",
    "    head_turn_threshold = 10.0  # Increased angle threshold for head turns (degrees)\n",
    "\n",
    "    hand_movement = (avg_left_hand is not None and avg_left_hand > hand_threshold) or (avg_right_hand is not None and avg_right_hand > hand_threshold)\n",
    "    shoulder_movement = avg_shoulder is not None and avg_shoulder > shoulder_threshold\n",
    "    head_movement = avg_head_turn is not None and avg_head_turn > head_turn_threshold\n",
    "\n",
    "    if hand_movement and shoulder_movement and head_movement:\n",
    "        return 'inspired'\n",
    "    elif hand_movement and (shoulder_movement or head_movement):\n",
    "        return 'interactive'\n",
    "    else:\n",
    "        return 'attentive'\n",
    "\n",
    "def process_frame(frame, frame_index):\n",
    "    global left_hand, right_hand, shoulder_midpoints, head_turn_angles, engagement_level\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    # Draw landmarks and calculate distances for left and right hands\n",
    "    left_hand_sum_distance = 0\n",
    "    right_hand_sum_distance = 0\n",
    "\n",
    "    if hand_results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    if hand_results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        shoulder_midpoint = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint)\n",
    "\n",
    "        left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE]\n",
    "        right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "        # Calculate head turn angle\n",
    "        if left_eye and right_eye and nose:\n",
    "            eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "            nose_vector = np.array([nose.x - (left_eye.x + right_eye.x) / 2, nose.y - (left_eye.y + right_eye.y) / 2])\n",
    "            cosine_angle = np.dot(eye_line_vector, nose_vector) / (np.linalg.norm(eye_line_vector) * np.linalg.norm(nose_vector))\n",
    "            head_turn_angle = np.arccos(cosine_angle) * (180 / np.pi)\n",
    "            head_turn_angles.append(head_turn_angle)\n",
    "        else:\n",
    "            head_turn_angles.append(None)\n",
    "    else:\n",
    "        shoulder_midpoints.append(None)\n",
    "        head_turn_angles.append(None)\n",
    "\n",
    "    # Calculate engagement level\n",
    "    engagement = calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles)\n",
    "    engagement_level.append(engagement)\n",
    "\n",
    "    # Filter out None values for graphs\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "    filtered_head_turn_angles = [point for point in head_turn_angles if point is not None]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "\n",
    "    # Calculate min and max values for y-axis limits\n",
    "    min_left_hand = min(filtered_left_hand) if filtered_left_hand else 0\n",
    "    max_left_hand = max(filtered_left_hand) if filtered_left_hand else 1\n",
    "    min_right_hand = min(filtered_right_hand) if filtered_right_hand else 0\n",
    "    max_right_hand = max(filtered_right_hand) if filtered_right_hand else 1\n",
    "    min_shoulder_midpoint = min(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 0\n",
    "    max_shoulder_midpoint = max(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 1\n",
    "    min_head_turn_angle = min(filtered_head_turn_angles) if filtered_head_turn_angles else 0\n",
    "    max_head_turn_angle = max(filtered_head_turn_angles) if filtered_head_turn_angles else 1\n",
    "    min_engagement = 1\n",
    "    max_engagement = 3\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    plt.figure(figsize=(output_width / 100, 4 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    plt.subplot(4, 1, 1)\n",
    "    if len(filtered_left_hand) > 0 and len(filtered_right_hand) > 0:\n",
    "        plt.plot(np.arange(len(filtered_left_hand)), filtered_left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "        plt.plot(np.arange(len(filtered_right_hand)), filtered_right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Hand Movements', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Shoulder Midpoints Graph\n",
    "    plt.subplot(4, 1, 2)\n",
    "    if len(filtered_shoulder_midpoints) > 0:\n",
    "        plt.plot(np.arange(len(filtered_shoulder_midpoints)), filtered_shoulder_midpoints, color='orange', linewidth=line_width, marker='o', markersize=1, label='Shoulder Midpoints')\n",
    "    plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Shoulder Midpoints', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Head Turn Angles Graph\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(filtered_head_turn_angles) > 0:\n",
    "        plt.plot(np.arange(len(filtered_head_turn_angles)), filtered_head_turn_angles, color='purple', linewidth=line_width, marker='o', markersize=1, label='Head Turn Angles')\n",
    "    plt.ylim([min_head_turn_angle, max_head_turn_angle])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Head Turn Angles', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Engagement Level Graph\n",
    "    engagement_colors = {'attentive': 'green', 'interactive': 'blue', 'inspired': 'red'}\n",
    "    engagement_numeric = [1 if e == 'attentive' else 2 if e == 'interactive' else 3 for e in filtered_engagement_level]\n",
    "    plt.subplot(4, 1, 4)\n",
    "    if len(filtered_engagement_level) > 0:\n",
    "        plt.plot(np.arange(len(filtered_engagement_level)), engagement_numeric, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "        for i, level in enumerate(filtered_engagement_level):\n",
    "            plt.plot(i, engagement_numeric[i], color=engagement_colors[level], marker='o')\n",
    "    plt.ylim([0.5, 3.5])\n",
    "    plt.yticks([1, 2, 3], ['Attentive', 'Interactive', 'Inspired'])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Engagement Level', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the graph with labels\n",
    "    plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.clf()\n",
    "\n",
    "    graph = cv2.imread('temp_graph.png')\n",
    "    graph = cv2.resize(graph, (output_width, 4 * graph_height))\n",
    "\n",
    "    return frame, graph\n",
    "\n",
    "def main():\n",
    "    global frame_index\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            future = executor.submit(process_frame, frame, frame_index)\n",
    "            processed_frame, graph = future.result()\n",
    "\n",
    "            # Update the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Display the input video\n",
    "            cv2.imshow('Input Video', processed_frame)\n",
    "\n",
    "            # Display the graph with labels\n",
    "            cv2.imshow('Graph', graph)\n",
    "\n",
    "            # Write the frame with graph to the output video\n",
    "            out.write(cv2.resize(processed_frame, (output_width, output_height)))\n",
    "\n",
    "            # Break the loop if the 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # Slow down the frame rate\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Release video objects and close OpenCV windows\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Pose and Holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles = []\n",
    "engagement_level = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, 20, (output_width, output_height))\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "\n",
    "def calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles):\n",
    "    # Use recent history to determine engagement level\n",
    "    history_len = 10  # Number of frames to consider for engagement calculation\n",
    "\n",
    "    # Slicing the recent history\n",
    "    recent_left_hand = left_hand[-history_len:]\n",
    "    recent_right_hand = right_hand[-history_len:]\n",
    "    recent_shoulder_midpoints = shoulder_midpoints[-history_len:]\n",
    "    recent_head_turn_angles = head_turn_angles[-history_len:]\n",
    "\n",
    "    # Calculate average movements\n",
    "    avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
    "    avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
    "    avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
    "    avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
    "\n",
    "    # Engagement calculation based on movement thresholds\n",
    "    hand_threshold = 0.1  # Movement threshold for hands\n",
    "    shoulder_threshold = 0.05  # Movement threshold for shoulders\n",
    "    head_turn_threshold = 5.0  # Angle threshold for head turns (degrees)\n",
    "\n",
    "    left_hand_movement = avg_left_hand is not None and avg_left_hand > hand_threshold\n",
    "    right_hand_movement = avg_right_hand is not None and avg_right_hand > hand_threshold\n",
    "    shoulder_movement = avg_shoulder is not None and avg_shoulder > shoulder_threshold\n",
    "    head_movement = avg_head_turn is not None and avg_head_turn > head_turn_threshold\n",
    "\n",
    "    if left_hand_movement and right_hand_movement:\n",
    "        return 'inspired'\n",
    "    elif left_hand_movement or right_hand_movement:\n",
    "        return 'interactive'\n",
    "    else:\n",
    "        return 'attentive'\n",
    "\n",
    "def process_frame(frame, frame_index):\n",
    "    global left_hand, right_hand, shoulder_midpoints, head_turn_angles, engagement_level\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    # Draw landmarks and calculate distances for left and right hands\n",
    "    left_hand_sum_distance = 0\n",
    "    right_hand_sum_distance = 0\n",
    "\n",
    "    if hand_results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    if hand_results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        shoulder_midpoint = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint)\n",
    "\n",
    "        left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE]\n",
    "        right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "        # Calculate head turn angle\n",
    "        if left_eye and right_eye and nose:\n",
    "            eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "            nose_vector = np.array([nose.x - (left_eye.x + right_eye.x) / 2, nose.y - (left_eye.y + right_eye.y) / 2])\n",
    "            cosine_angle = np.dot(eye_line_vector, nose_vector) / (np.linalg.norm(eye_line_vector) * np.linalg.norm(nose_vector))\n",
    "            head_turn_angle = np.arccos(cosine_angle) * (180 / np.pi)\n",
    "            head_turn_angles.append(head_turn_angle)\n",
    "        else:\n",
    "            head_turn_angles.append(None)\n",
    "    else:\n",
    "        shoulder_midpoints.append(None)\n",
    "        head_turn_angles.append(None)\n",
    "\n",
    "    # Calculate engagement level\n",
    "    engagement = calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles)\n",
    "    engagement_level.append(engagement)\n",
    "\n",
    "    # Filter out None values for graphs\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "    filtered_head_turn_angles = [point for point in head_turn_angles if point is not None]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "\n",
    "    # Calculate min and max values for y-axis limits\n",
    "    min_left_hand = min(filtered_left_hand) if filtered_left_hand else 0\n",
    "    max_left_hand = max(filtered_left_hand) if filtered_left_hand else 1\n",
    "    min_right_hand = min(filtered_right_hand) if filtered_right_hand else 0\n",
    "    max_right_hand = max(filtered_right_hand) if filtered_right_hand else 1\n",
    "    min_shoulder_midpoint = min(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 0\n",
    "    max_shoulder_midpoint = max(filtered_shoulder_midpoints) if filtered_shoulder_midpoints else 1\n",
    "    min_head_turn_angle = min(filtered_head_turn_angles) if filtered_head_turn_angles else 0\n",
    "    max_head_turn_angle = max(filtered_head_turn_angles) if filtered_head_turn_angles else 1\n",
    "    min_engagement = 1\n",
    "    max_engagement = 3\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    plt.figure(figsize=(output_width / 100, 4 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    plt.subplot(4, 1, 1)\n",
    "    if len(filtered_left_hand) > 0 and len(filtered_right_hand) > 0:\n",
    "        plt.plot(np.arange(len(filtered_left_hand)), filtered_left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "        plt.plot(np.arange(len(filtered_right_hand)), filtered_right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Hand Movements', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Shoulder Midpoints Graph\n",
    "    plt.subplot(4, 1, 2)\n",
    "    if len(filtered_shoulder_midpoints) > 0:\n",
    "        plt.plot(np.arange(len(filtered_shoulder_midpoints)), filtered_shoulder_midpoints, color='orange', linewidth=line_width, marker='o', markersize=1, label='Shoulder Midpoints')\n",
    "    plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Shoulder Midpoints', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Head Turn Angles Graph\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(filtered_head_turn_angles) > 0:\n",
    "        plt.plot(np.arange(len(filtered_head_turn_angles)), filtered_head_turn_angles, color='purple', linewidth=line_width, marker='o', markersize=1, label='Head Turn Angles')\n",
    "    plt.ylim([min_head_turn_angle, max_head_turn_angle])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Head Turn Angles', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Engagement Level Graph\n",
    "    engagement_colors = {'attentive': 'green', 'interactive': 'blue', 'inspired': 'red'}\n",
    "    engagement_numeric = [1 if e == 'attentive' else 2 if e == 'interactive' else 3 for e in filtered_engagement_level]\n",
    "    plt.subplot(4, 1, 4)\n",
    "    if len(filtered_engagement_level) > 0:\n",
    "        plt.plot(np.arange(len(filtered_engagement_level)), engagement_numeric, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "        for i, level in enumerate(filtered_engagement_level):\n",
    "            plt.plot(i, engagement_numeric[i], color=engagement_colors[level], marker='o')\n",
    "    plt.ylim([0.5, 3.5])\n",
    "    plt.yticks([1, 2, 3], ['Attentive', 'Interactive', 'Inspired'])\n",
    "    plt.xlim([0, frame_index])\n",
    "    plt.axis('on')\n",
    "    plt.ylabel('Engagement Level', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the graph with labels\n",
    "    plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.clf()\n",
    "\n",
    "    graph = cv2.imread('temp_graph.png')\n",
    "    graph = cv2.resize(graph, (output_width, 4 * graph_height))\n",
    "\n",
    "    return frame, graph\n",
    "\n",
    "def main():\n",
    "    global frame_index\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            future = executor.submit(process_frame, frame, frame_index)\n",
    "            processed_frame, graph = future.result()\n",
    "\n",
    "            # Update the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Display the input video\n",
    "            cv2.imshow('Input Video', processed_frame)\n",
    "\n",
    "            # Display the graph with labels\n",
    "            cv2.imshow('Graph', graph)\n",
    "\n",
    "            # Write the frame with graph to the output video\n",
    "            out.write(cv2.resize(processed_frame, (output_width, output_height)))\n",
    "\n",
    "            # Break the loop if the 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # Slow down the frame rate\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Release video objects and close OpenCV windows\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code is with streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 11:26:21.280079: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 11:26:21.285223: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 11:26:21.349391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 11:26:22.610230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719208584.650944    7031 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1719208584.656879    8104 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.5-1ubuntu1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 17.0.6, DRM 3.57, 6.8.0-35-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1719208584.694374    7031 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1719208584.696845    8124 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.5-1ubuntu1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 17.0.6, DRM 3.57, 6.8.0-35-generic)\n",
      "W0000 00:00:1719208584.781980    8079 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.818376    8107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.819894    8081 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.858422    8111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.862900    8109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.862977    8107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.864517    8112 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.871279    8111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.894098    8108 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1719208584.894098    8107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-06-24 11:26:25.102 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/rudra/myenv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:57: RuntimeWarning: Mean of empty slice\n",
      "  avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:58: RuntimeWarning: Mean of empty slice\n",
      "  avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:59: RuntimeWarning: Mean of empty slice\n",
      "  avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
      "/tmp/ipykernel_7031/1712597196.py:60: RuntimeWarning: Mean of empty slice\n",
      "  avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe Pose and Holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize lists to store data for graphs\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles = []\n",
    "engagement_level = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200\n",
    "line_width = 1\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800\n",
    "output_height = 600\n",
    "\n",
    "# Streamlit interface\n",
    "st.title(\"Real-time Pose and Engagement Analysis\")\n",
    "st.sidebar.title(\"Controls\")\n",
    "\n",
    "# Camera toggle\n",
    "camera_enabled = st.sidebar.checkbox(\"Enable Camera\", value=True)\n",
    "\n",
    "# Create a video capture object\n",
    "cap = None\n",
    "if camera_enabled:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "def calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles):\n",
    "    # Use recent history to determine engagement level\n",
    "    history_len = 10  # Number of frames to consider for engagement calculation\n",
    "\n",
    "    # Slicing the recent history\n",
    "    recent_left_hand = left_hand[-history_len:]\n",
    "    recent_right_hand = right_hand[-history_len:]\n",
    "    recent_shoulder_midpoints = shoulder_midpoints[-history_len:]\n",
    "    recent_head_turn_angles = head_turn_angles[-history_len:]\n",
    "\n",
    "    # Calculate average movements\n",
    "    avg_left_hand = np.nanmean([x for x in recent_left_hand if x is not None])\n",
    "    avg_right_hand = np.nanmean([x for x in recent_right_hand if x is not None])\n",
    "    avg_shoulder = np.nanmean([x for x in recent_shoulder_midpoints if x is not None])\n",
    "    avg_head_turn = np.nanmean([x for x in recent_head_turn_angles if x is not None])\n",
    "\n",
    "    # Engagement calculation based on movement thresholds\n",
    "    hand_threshold = 0.1  # Movement threshold for hands\n",
    "    shoulder_threshold = 0.05  # Movement threshold for shoulders\n",
    "    head_turn_threshold = 5.0  # Angle threshold for head turns (degrees)\n",
    "\n",
    "    left_hand_movement = avg_left_hand is not None and avg_left_hand > hand_threshold\n",
    "    right_hand_movement = avg_right_hand is not None and avg_right_hand > hand_threshold\n",
    "    shoulder_movement = avg_shoulder is not None and avg_shoulder > shoulder_threshold\n",
    "    head_movement = avg_head_turn is not None and avg_head_turn > head_turn_threshold\n",
    "\n",
    "    if left_hand_movement and right_hand_movement:\n",
    "        return 'inspired'\n",
    "    elif left_hand_movement or right_hand_movement:\n",
    "        return 'interactive'\n",
    "    else:\n",
    "        return 'attentive'\n",
    "\n",
    "def process_frame(frame, frame_index):\n",
    "    global left_hand, right_hand, shoulder_midpoints, head_turn_angles, engagement_level\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "    # Detect pose in the frame\n",
    "    results = pose.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    hand_results = holistic_model.process(frame_rgb)\n",
    "    frame_rgb.flags.writeable = True\n",
    "\n",
    "    # Draw landmarks and calculate distances for left and right hands\n",
    "    left_hand_sum_distance = 0\n",
    "    right_hand_sum_distance = 0\n",
    "\n",
    "    if hand_results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "        for landmark in left_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            left_hand_sum_distance += distance\n",
    "        left_hand.append(left_hand_sum_distance)\n",
    "    else:\n",
    "        left_hand.append(None)\n",
    "\n",
    "    if hand_results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "        for landmark in right_hand_landmarks:\n",
    "            x = landmark.x\n",
    "            y = landmark.y\n",
    "            z = landmark.z\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            right_hand_sum_distance += distance\n",
    "        right_hand.append(right_hand_sum_distance)\n",
    "    else:\n",
    "        right_hand.append(None)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        shoulder_midpoint = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint)\n",
    "\n",
    "        left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE]\n",
    "        right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "        # Calculate head turn angle\n",
    "        if left_eye and right_eye and nose:\n",
    "            eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "            nose_vector = np.array([nose.x - (left_eye.x + right_eye.x) / 2, nose.y - (left_eye.y + right_eye.y) / 2])\n",
    "            cosine_angle = np.dot(eye_line_vector, nose_vector) / (np.linalg.norm(eye_line_vector) * np.linalg.norm(nose_vector))\n",
    "            head_turn_angle = np.arccos(cosine_angle) * (180 / np.pi)\n",
    "            head_turn_angles.append(head_turn_angle)\n",
    "        else:\n",
    "            head_turn_angles.append(None)\n",
    "    else:\n",
    "        shoulder_midpoints.append(None)\n",
    "        head_turn_angles.append(None)\n",
    "\n",
    "    # Calculate engagement level\n",
    "    engagement = calculate_engagement(left_hand, right_hand, shoulder_midpoints, head_turn_angles)\n",
    "    engagement_level.append(engagement)\n",
    "\n",
    "    # Filter out None values for graphs\n",
    "    filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "    filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "    filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "    filtered_head_turn_angles = [point for point in head_turn_angles if point is not None]\n",
    "    filtered_engagement_level = [e for e in engagement_level if e is not None]\n",
    "\n",
    "    # Create separate figures for each graph\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(output_width / 100, 4 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "    # Hand Movement Graph\n",
    "    axes[0].plot(filtered_left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "    axes[0].plot(filtered_right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "    axes[0].set_ylim([min(filtered_left_hand, default=0), max(filtered_left_hand, default=1)])\n",
    "    axes[0].set_ylabel('Hand Movements')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Shoulder Midpoints Graph\n",
    "    axes[1].plot(filtered_shoulder_midpoints, color='orange', linewidth=line_width, marker='o', markersize=1, label='Shoulder Midpoints')\n",
    "    axes[1].set_ylim([min(filtered_shoulder_midpoints, default=0), max(filtered_shoulder_midpoints, default=1)])\n",
    "    axes[1].set_ylabel('Shoulder Midpoints')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Head Turn Angles Graph\n",
    "    axes[2].plot(filtered_head_turn_angles, color='purple', linewidth=line_width, marker='o', markersize=1, label='Head Turn Angles')\n",
    "    axes[2].set_ylim([min(filtered_head_turn_angles, default=0), max(filtered_head_turn_angles, default=1)])\n",
    "    axes[2].set_ylabel('Head Turn Angles')\n",
    "    axes[2].legend()\n",
    "\n",
    "    # Engagement Level Graph\n",
    "    engagement_colors = {'attentive': 'green', 'interactive': 'blue', 'inspired': 'red'}\n",
    "    engagement_numeric = [1 if e == 'attentive' else 2 if e == 'interactive' else 3 for e in filtered_engagement_level]\n",
    "    axes[3].plot(engagement_numeric, color='green', linewidth=line_width, marker='o', markersize=1, label='Engagement Level')\n",
    "    for i, level in enumerate(filtered_engagement_level):\n",
    "        axes[3].plot(i, engagement_numeric[i], color=engagement_colors[level], marker='o')\n",
    "    axes[3].set_ylim([0.5, 3.5])\n",
    "    axes[3].set_yticks([1, 2, 3])\n",
    "    axes[3].set_yticklabels(['Attentive', 'Interactive', 'Inspired'])\n",
    "    axes[3].set_ylabel('Engagement Level')\n",
    "    axes[3].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return frame, fig\n",
    "\n",
    "# Main loop\n",
    "def main():\n",
    "    global frame_index\n",
    "\n",
    "    frame_index = 0\n",
    "\n",
    "    if camera_enabled:\n",
    "        stframe = st.empty()\n",
    "        graphframe = st.empty()\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                future = executor.submit(process_frame, frame, frame_index)\n",
    "                processed_frame, fig = future.result()\n",
    "\n",
    "                frame_index += 1\n",
    "\n",
    "                # Display the frame\n",
    "                stframe.image(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                # Display the graph\n",
    "                graphframe.pyplot(fig)\n",
    "\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
