{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 15:20:22.678156: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 15:20:23.528156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Declaring Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@3.630] global cap.cpp:164 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.9.0) /io/opencv/modules/videoio/src/cap_images.cpp:300: error: (-215:Assertion failed) !_filename.empty() in function 'open'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m     15\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m video_length_seconds \u001b[38;5;241m=\u001b[39m \u001b[43mframe_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize MediaPipe Pose\u001b[39;00m\n\u001b[1;32m     19\u001b[0m mp_pose \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mpose\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the video\n",
    "\n",
    "#load your video path here, test_load_path\n",
    "#test_load_path\n",
    "input_video = ''\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_count = int(cap.get(7))\n",
    "fps = int(cap.get(5))\n",
    "video_length_seconds = frame_count / fps\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize MediaPipe holistics\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "# Load pre-trained emotion detection model\n",
    "emotion_model = load_model('emotion_model.h5')  # Load your emotion detection model here\n",
    "\n",
    "# Create empty lists to store data for graphs\n",
    "shoulder_midpoints = []\n",
    "head_turn_angles_right = []\n",
    "head_turn_angles_left = []\n",
    "emotions = []  # List to store detected emotions\n",
    "emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "\n",
    "# Set the graph display parameters\n",
    "graph_height = 200  # Height of the graph (increased)\n",
    "graph_color = (0, 0, 0)  # Color of the graph (BGR format)\n",
    "line_width = 1  # Width of the plotted line\n",
    "\n",
    "# Set video dimensions\n",
    "output_width = 800  # Adjust the width as needed\n",
    "output_height = int(frame_height * (output_width / frame_width))\n",
    "\n",
    "# Create an output video writer\n",
    "output_video = 'video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (output_width, output_height))  # Adjust dimensions\n",
    "\n",
    "# Initialize frame index\n",
    "frame_index = 0\n",
    "frame_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function declaration</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate speaker position relative to the center\n",
    "def calculate_speaker_position(frame, shoulder_midpoint_x):\n",
    "    frame_width = frame.shape[1]\n",
    "    speaker_position = shoulder_midpoint_x - frame_width / 2\n",
    "    return speaker_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing each frame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_counter += 1\n",
    "    # Convert the frame to RGB (MediaPipe requires RGB input)\n",
    "    if frame_counter >= fps/6:\n",
    "        frame_counter = 0\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "        # Detect emotions in the frame and append to the emotions list\n",
    "        try:\n",
    "            for t in range(1):\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Frame rectangle\n",
    "                    cv2.rectangle(frame, pt1=(x, y), pt2=(x + w, y + h), color=(0, 0, 255), thickness=2)\n",
    "                    roi_gray = frame_grey[y - 5:y + h + 5, x - 5:x + w + 5]\n",
    "                    roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "                    image_pixels = img_to_array(roi_gray)\n",
    "                    image_pixels = np.expand_dims(image_pixels, axis=0)\n",
    "                    # Normalize\n",
    "                    image_pixels /= 255\n",
    "                    predictions = emotion_model.predict(image_pixels)\n",
    "                    max_index = np.argmax(predictions[0])\n",
    "                    detected_emotion = emotion_labels[max_index]\n",
    "                    emotions.append(detected_emotion)\n",
    "        except:\n",
    "            emotions.append(None)\n",
    "\n",
    "        # Detect pose in the frame\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks is not None:\n",
    "            # Extract landmarks of the left and right shoulders (ids 11 and 12)\n",
    "            left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "            right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "            if left_shoulder and right_shoulder:\n",
    "                # Calculate the midpoint of the shoulders\n",
    "                shoulder_midpoint_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "\n",
    "                # Append the shoulder midpoint to the list\n",
    "                shoulder_midpoints.append(shoulder_midpoint_x)\n",
    "            else:\n",
    "                # Append None for frames with no shoulder points\n",
    "                shoulder_midpoints.append(None)\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Extract landmarks of the left and right eyes and nose\n",
    "            left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "            right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "            nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "            if left_eye and right_eye and nose:\n",
    "                # Calculate the angle between the line connecting the eyes and a horizontal reference line\n",
    "                eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "                eye_left_nose_vector = np.array([nose.x-left_eye.x ,  nose.y-left_eye.y])\n",
    "                eye_right_nose_vector = np.array([right_eye.x - nose.x, right_eye.y - nose.y])\n",
    "\n",
    "                dot_product_left = np.dot(eye_line_vector, eye_left_nose_vector)\n",
    "                eye_line_magnitude = np.linalg.norm(eye_line_vector)\n",
    "                eye_left_nose_magnitude = np.linalg.norm(eye_left_nose_vector)\n",
    "\n",
    "                dot_product_right = np.dot(eye_line_vector, eye_right_nose_vector)\n",
    "                eye_right_nose_magnitude = np.linalg.norm(eye_right_nose_vector)\n",
    "\n",
    "                cosine_angle_left = dot_product_left / (eye_line_magnitude * eye_left_nose_magnitude)\n",
    "                cosine_angle_right = dot_product_right / (eye_line_magnitude * eye_right_nose_magnitude)\n",
    "\n",
    "                head_turn_angle_left = np.arccos(cosine_angle_left) * (180 / np.pi)\n",
    "                head_turn_angle_right = np.arccos(cosine_angle_right) * (180 / np.pi)\n",
    "\n",
    "                head_turn_angles_left.append(head_turn_angle_left)\n",
    "                head_turn_angles_right.append(head_turn_angle_right)\n",
    "\n",
    "            else:\n",
    "                head_turn_angles_left.append(None)\n",
    "                head_turn_angles_right.append(None)\n",
    "        else:\n",
    "            shoulder_midpoints.append(None)\n",
    "            head_turn_angles_left.append(None)\n",
    "            head_turn_angles_right.append(None)\n",
    "\n",
    "        # Update the frame index\n",
    "        frame_index += 1\n",
    "\n",
    "        # Check if the shoulder_midpoints list is not empty\n",
    "        if shoulder_midpoints:\n",
    "            # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "            filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "            min_shoulder_midpoint = min(filtered_shoulder_midpoints)\n",
    "            max_shoulder_midpoint = max(filtered_shoulder_midpoints)\n",
    "        else:\n",
    "            # If the list is empty, set min and max to 0\n",
    "            min_shoulder_midpoint = 0\n",
    "            max_shoulder_midpoint = 0\n",
    "\n",
    "        if head_turn_angles_left:\n",
    "            # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "            filtered_head_turn_angles_left = [angle for angle in head_turn_angles_left if angle is not None]\n",
    "            min_head_turn_angle_left = min(filtered_head_turn_angles_left)\n",
    "            max_head_turn_angle_left = max(filtered_head_turn_angles_left)\n",
    "        else:\n",
    "            min_head_turn_angle_left = 0\n",
    "            max_head_turn_angle_left = 0\n",
    "        \n",
    "        if head_turn_angles_right:\n",
    "            # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "            filtered_head_turn_angles_right = [angle for angle in head_turn_angles_right if angle is not None]\n",
    "            min_head_turn_angle_right = min(filtered_head_turn_angles_right)\n",
    "            max_head_turn_angle_right = max(filtered_head_turn_angles_right)\n",
    "        else:\n",
    "            min_head_turn_angle_right = 0\n",
    "            max_head_turn_angle_right = 0\n",
    "        \n",
    "        frame_rgb.flags.writeable = False\n",
    "        hand_results = holistic_model.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "\n",
    "        left_hand_sum_distance = 0\n",
    "        if hand_results.left_hand_landmarks is not None:\n",
    "            # Drawing Left hand Land Marks\n",
    "            # mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            mp_drawing.draw_landmarks( frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "            for landmark in left_hand_landmarks:\n",
    "                x = landmark.x\n",
    "                y = landmark.y\n",
    "                z = landmark.z\n",
    "                distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "                left_hand_sum_distance += distance\n",
    "                \n",
    "            left_hand.append(left_hand_sum_distance)\n",
    "        else:\n",
    "            left_hand.append(None)\n",
    "        \n",
    "        right_hand_sum_distance = 0\n",
    "        if hand_results.right_hand_landmarks is not None:\n",
    "            mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS )\n",
    "            right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "            for landmark in right_hand_landmarks:\n",
    "                x = landmark.x\n",
    "                y = landmark.y\n",
    "                z = landmark.z\n",
    "                distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "                right_hand_sum_distance += distance\n",
    "            right_hand.append(right_hand_sum_distance)\n",
    "        else:\n",
    "            right_hand.append(None)\n",
    "\n",
    "        filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "        if filtered_left_hand:\n",
    "            # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "            min_left_hand = min(filtered_left_hand)\n",
    "            max_left_hand = max(filtered_left_hand)\n",
    "        else:\n",
    "            # If the list is empty, set min and max to 0\n",
    "            min_left_hand = 0\n",
    "            max_left_hand = 0\n",
    "\n",
    "        filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "        if filtered_right_hand:\n",
    "            # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "            min_right_hand = min(filtered_right_hand)\n",
    "            max_right_hand = max(filtered_right_hand)\n",
    "        else:\n",
    "            # If the list is empty, set min and max to 0\n",
    "            min_right_hand = 0\n",
    "            max_right_hand = 0\n",
    "\n",
    "\n",
    "        # Create separate figures for each graph\n",
    "        plt.figure(figsize=(output_width / 100, 3 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "        # Shoulder Movement Graph\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.plot(np.arange(frame_index), shoulder_midpoints, color='red', linewidth=line_width, marker='o', markersize=1)\n",
    "        plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "        plt.xlim([0, video_length_seconds*6])\n",
    "        plt.axis('on')\n",
    "        plt.ylabel('Shoulder Midpoint', fontsize=12)\n",
    "\n",
    "        # Head Turn Angle Graph\n",
    "        plt.subplot(4, 1, 2)\n",
    "        plt.plot(np.arange(frame_index), head_turn_angles_left, color='red', linewidth=line_width, marker='o', markersize=1, label='Left Turn Angle')\n",
    "        plt.plot(np.arange(frame_index), head_turn_angles_right, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right Turn Angle')\n",
    "        plt.ylim([min(min_head_turn_angle_left, min_head_turn_angle_right), max(max_head_turn_angle_left, max_head_turn_angle_right)])\n",
    "        plt.xlim([0, video_length_seconds*6])\n",
    "        plt.axis('on')\n",
    "        plt.ylabel('Head Turn Angle (degrees)', fontsize=12)\n",
    "        plt.legend()\n",
    "\n",
    "        # Emotion Graph\n",
    "        plt.subplot(4, 1, 3)\n",
    "        x_values = np.arange(len(emotions))  # Use the length of the emotions list as x-axis values\n",
    "        plt.scatter(x_values, [emotion_labels.index(e) if e is not None else -1 for e in emotions],\n",
    "                    color='green', s=5)  # Plot emotions as dots\n",
    "        plt.ylim(-1, len(emotion_labels))  # Set y-axis limits\n",
    "        plt.xlim(0, video_length_seconds*6)  # Set x-axis limits\n",
    "        plt.yticks(range(len(emotion_labels)), emotion_labels)  # Set y-tick labels\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)  # Add gridlines\n",
    "        plt.ylabel('Emotion', fontsize=12)  # Add y-axis label\n",
    "\n",
    "        # Hand Graph\n",
    "        plt.subplot(4, 1, 4)\n",
    "        plt.plot(np.arange(frame_index), left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "        plt.plot(np.arange(frame_index), right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "        plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "        plt.xlim([0, video_length_seconds*6])\n",
    "        plt.axis('on')\n",
    "        plt.ylabel('Hand movements', fontsize=12)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the graph with labels\n",
    "        plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        plt.clf()\n",
    "\n",
    "        graph = cv2.imread('temp_graph.png')\n",
    "\n",
    "        # Resize the graph to match the video width and increased height\n",
    "        graph = cv2.resize(graph, (output_width, 3 * graph_height))\n",
    "\n",
    "        # Display the input video\n",
    "        cv2.imshow('Input Video', frame)\n",
    "\n",
    "        # Display the graph with labels\n",
    "        cv2.imshow('Graph', graph)\n",
    "\n",
    "        # Write the frame with graph to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video objects and close OpenCV windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
