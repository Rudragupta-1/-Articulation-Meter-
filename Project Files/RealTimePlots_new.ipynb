{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from pytube import YouTube\n",
    "import os\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function declaration</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate speaker position relative to the center\n",
    "def calculate_speaker_position(frame, shoulder_midpoint_x):\n",
    "    frame_width = frame.shape[1]\n",
    "    speaker_position = shoulder_midpoint_x - frame_width / 2\n",
    "    return speaker_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Process Each Frame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install yt-dlp moviepy opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pafy opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# updated file\n",
    "import os\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "from yt_dlp import YoutubeDL\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# excel_file = 'TED_Dataset.xlsx'\n",
    "# sheet_name = 'TED_Dataset'\n",
    "# df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "# video_links = df['youtube_video_code']\n",
    "# Load the video\n",
    "for video_link in range(1):\n",
    "    youtube_url = \"https://youtu.be/0EykDvxJAyA?si=mbwfqH0E1Qp1TAJe\"\n",
    "\n",
    "    # Download the video using yt-dlp\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': 'downloaded_video.%(ext)s',\n",
    "    }\n",
    "\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(youtube_url, download=True)\n",
    "        video_file = ydl.prepare_filename(info_dict)\n",
    "\n",
    "    # Load the video using moviepy\n",
    "    clip = VideoFileClip(video_file)\n",
    "\n",
    "    # Open the video file using OpenCV\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_count = int(cap.get(7))\n",
    "    fps = int(cap.get(5))\n",
    "    video_length_seconds = frame_count / fps\n",
    "\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Initialize MediaPipe holistics\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic_model = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    # Load pre-trained emotion detection model\n",
    "    emotion_model = load_model('emotion_model.h5')  # Load your emotion detection model here\n",
    "\n",
    "    # Create empty lists to store data for graphs\n",
    "    shoulder_midpoints = []\n",
    "    head_turn_angles_right = []\n",
    "    head_turn_angles_left = []\n",
    "    emotions = []  # List to store detected emotions\n",
    "    emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "    left_hand = []\n",
    "    right_hand = []\n",
    "\n",
    "    # Set the graph display parameters\n",
    "    graph_height = 200  # Height of the graph (increased)\n",
    "    graph_color = (0, 0, 0)  # Color of the graph (BGR format)\n",
    "    line_width = 1  # Width of the plotted line\n",
    "\n",
    "    # Set video dimensions\n",
    "    output_width = 800  # Adjust the width as needed\n",
    "    output_height = int(frame_height * (output_width / frame_width))\n",
    "\n",
    "    # Create an output video writer\n",
    "    output_video = 'output_video.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (output_width, output_height))  # Adjust dimensions\n",
    "\n",
    "    # Initialize frame index\n",
    "    frame_index = 0\n",
    "    frame_counter = 0\n",
    "    # Process each frame\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_counter += 1\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB input)\n",
    "        if frame_counter >= fps/6:\n",
    "            frame_counter = 0\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "\n",
    "            # Detect emotions in the frame and append to the emotions list\n",
    "            try:\n",
    "                for t in range(1):\n",
    "                    for (x, y, w, h) in faces:\n",
    "                        # Frame rectangle\n",
    "                        cv2.rectangle(frame, pt1=(x, y), pt2=(x + w, y + h), color=(0, 0, 255), thickness=2)\n",
    "                        roi_gray = frame_grey[y - 5:y + h + 5, x - 5:x + w + 5]\n",
    "                        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "                        image_pixels = img_to_array(roi_gray)\n",
    "                        image_pixels = np.expand_dims(image_pixels, axis=0)\n",
    "                        # Normalize\n",
    "                        image_pixels /= 255\n",
    "                        predictions = emotion_model.predict(image_pixels)\n",
    "                        max_index = np.argmax(predictions[0])\n",
    "                        detected_emotion = emotion_labels[max_index]\n",
    "                        emotions.append(detected_emotion)\n",
    "                        print(emotions)\n",
    "                        print(\"\\n\")\n",
    "            except:\n",
    "                emotions.append(None)\n",
    "\n",
    "            # Detect pose in the frame\n",
    "            results = pose.process(frame_rgb)\n",
    "\n",
    "            if results.pose_landmarks is not None:\n",
    "                # Extract landmarks of the left and right shoulders (ids 11 and 12)\n",
    "                left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "                right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "                if left_shoulder and right_shoulder:\n",
    "                    # Calculate the midpoint of the shoulders\n",
    "                    shoulder_midpoint_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "\n",
    "                    # Append the shoulder midpoint to the list\n",
    "                    shoulder_midpoints.append(shoulder_midpoint_x)\n",
    "                else:\n",
    "                    # Append None for frames with no shoulder points\n",
    "                    shoulder_midpoints.append(None)\n",
    "                mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                # Extract landmarks of the left and right eyes and nose\n",
    "                left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "                right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "                nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "\n",
    "                if left_eye and right_eye and nose:\n",
    "                    # Calculate the angle between the line connecting the eyes and a horizontal reference line\n",
    "                    eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "                    eye_left_nose_vector = np.array([nose.x-left_eye.x ,  nose.y-left_eye.y])\n",
    "                    eye_right_nose_vector = np.array([right_eye.x - nose.x, right_eye.y - nose.y])\n",
    "\n",
    "                    dot_product_left = np.dot(eye_line_vector, eye_left_nose_vector)\n",
    "                    eye_line_magnitude = np.linalg.norm(eye_line_vector)\n",
    "                    eye_left_nose_magnitude = np.linalg.norm(eye_left_nose_vector)\n",
    "\n",
    "                    dot_product_right = np.dot(eye_line_vector, eye_right_nose_vector)\n",
    "                    eye_right_nose_magnitude = np.linalg.norm(eye_right_nose_vector)\n",
    "\n",
    "                    cosine_angle_left = dot_product_left / (eye_line_magnitude * eye_left_nose_magnitude)\n",
    "                    cosine_angle_right = dot_product_right / (eye_line_magnitude * eye_right_nose_magnitude)\n",
    "\n",
    "                    head_turn_angle_left = np.arccos(cosine_angle_left) * (180 / np.pi)\n",
    "                    head_turn_angle_right = np.arccos(cosine_angle_right) * (180 / np.pi)\n",
    "\n",
    "                    head_turn_angles_left.append(head_turn_angle_left)\n",
    "                    head_turn_angles_right.append(head_turn_angle_right)\n",
    "\n",
    "                else:\n",
    "                    head_turn_angles_left.append(None)\n",
    "                    head_turn_angles_right.append(None)\n",
    "            else:\n",
    "                shoulder_midpoints.append(None)\n",
    "                head_turn_angles_left.append(None)\n",
    "                head_turn_angles_right.append(None)\n",
    "\n",
    "            # Update the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Check if the shoulder_midpoints list is not empty\n",
    "            if shoulder_midpoints:\n",
    "                # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "                filtered_shoulder_midpoints = [point for point in shoulder_midpoints if point is not None]\n",
    "                if filtered_shoulder_midpoints:\n",
    "                    min_shoulder_midpoint = min(filtered_shoulder_midpoints)\n",
    "                    max_shoulder_midpoint = max(filtered_shoulder_midpoints)\n",
    "                else:\n",
    "                    # If the list is empty, set min and max to 0\n",
    "                    min_shoulder_midpoint = 0\n",
    "                    max_shoulder_midpoint = 0\n",
    "            else:\n",
    "                # If the list is empty, set min and max to 0\n",
    "                min_shoulder_midpoint = 0\n",
    "                max_shoulder_midpoint = 0\n",
    "\n",
    "            if head_turn_angles_left:\n",
    "                # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "                filtered_head_turn_angles_left = [angle for angle in head_turn_angles_left if angle is not None]\n",
    "                if filtered_head_turn_angles_left:\n",
    "                    min_head_turn_angle_left = min(filtered_head_turn_angles_left)\n",
    "                    max_head_turn_angle_left = max(filtered_head_turn_angles_left)\n",
    "                else:\n",
    "                    min_head_turn_angle_left = 0\n",
    "                    max_head_turn_angle_left = 0\n",
    "                \n",
    "            else:\n",
    "                min_head_turn_angle_left = 0\n",
    "                max_head_turn_angle_left = 0\n",
    "            \n",
    "            if head_turn_angles_right:\n",
    "                # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "                filtered_head_turn_angles_right = [angle for angle in head_turn_angles_right if angle is not None]\n",
    "                if filtered_head_turn_angles_right:\n",
    "                    min_head_turn_angle_right = min(filtered_head_turn_angles_right)\n",
    "                    max_head_turn_angle_right = max(filtered_head_turn_angles_right)\n",
    "                else:\n",
    "                    min_head_turn_angle_right = 0\n",
    "                    max_head_turn_angle_right = 0\n",
    "            else:\n",
    "                min_head_turn_angle_right = 0\n",
    "                max_head_turn_angle_right = 0\n",
    "            \n",
    "            frame_rgb.flags.writeable = False\n",
    "            hand_results = holistic_model.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "\n",
    "            left_hand_sum_distance = 0\n",
    "            if hand_results.left_hand_landmarks is not None:\n",
    "                # Drawing Left hand Land Marks\n",
    "                # mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                mp_drawing.draw_landmarks( frame, hand_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                left_hand_landmarks = hand_results.left_hand_landmarks.landmark\n",
    "                for landmark in left_hand_landmarks:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "                    distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "                    left_hand_sum_distance += distance\n",
    "                    \n",
    "                left_hand.append(left_hand_sum_distance)\n",
    "            else:\n",
    "                left_hand.append(None)\n",
    "            \n",
    "            right_hand_sum_distance = 0\n",
    "            if hand_results.right_hand_landmarks is not None:\n",
    "                mp_drawing.draw_landmarks(frame, hand_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS )\n",
    "                right_hand_landmarks = hand_results.right_hand_landmarks.landmark\n",
    "                for landmark in right_hand_landmarks:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "                    distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "                    right_hand_sum_distance += distance\n",
    "                right_hand.append(right_hand_sum_distance)\n",
    "            else:\n",
    "                right_hand.append(None)\n",
    "\n",
    "            filtered_left_hand = [point for point in left_hand if point is not None]\n",
    "            if filtered_left_hand:\n",
    "                # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "                min_left_hand = min(filtered_left_hand)\n",
    "                max_left_hand = max(filtered_left_hand)\n",
    "            else:\n",
    "                # If the list is empty, set min and max to 0\n",
    "                min_left_hand = 0\n",
    "                max_left_hand = 0\n",
    "\n",
    "            filtered_right_hand = [point for point in right_hand if point is not None]\n",
    "            if filtered_right_hand:\n",
    "                # Filter out None values and calculate the minimum and maximum values for each graph\n",
    "                min_right_hand = min(filtered_right_hand)\n",
    "                max_right_hand = max(filtered_right_hand)\n",
    "            else:\n",
    "                # If the list is empty, set min and max to 0\n",
    "                min_right_hand = 0\n",
    "                max_right_hand = 0\n",
    "\n",
    "\n",
    "            # Create separate figures for each graph\n",
    "            plt.figure(figsize=(output_width / 100, 3 * graph_height / 100))  # Adjust graph size\n",
    "\n",
    "            # Shoulder Movement Graph\n",
    "            plt.subplot(4, 1, 1)\n",
    "            plt.plot(np.arange(frame_index), shoulder_midpoints, color='red', linewidth=line_width, marker='o', markersize=1)\n",
    "            plt.ylim([min_shoulder_midpoint, max_shoulder_midpoint])\n",
    "            plt.xlim([0, video_length_seconds*6])\n",
    "            plt.axis('on')\n",
    "            plt.ylabel('Shoulder Midpoint', fontsize=12)\n",
    "\n",
    "            # Head Turn Angle Graph\n",
    "            plt.subplot(4, 1, 2)\n",
    "            plt.plot(np.arange(frame_index), head_turn_angles_left, color='red', linewidth=line_width, marker='o', markersize=1, label='Left Turn Angle')\n",
    "            plt.plot(np.arange(frame_index), head_turn_angles_right, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right Turn Angle')\n",
    "            plt.ylim([min(min_head_turn_angle_left, min_head_turn_angle_right), max(max_head_turn_angle_left, max_head_turn_angle_right)])\n",
    "            plt.xlim([0, video_length_seconds*6])\n",
    "            plt.axis('on')\n",
    "            plt.ylabel('Head Turn Angle (degrees)', fontsize=12)\n",
    "            plt.legend()\n",
    "\n",
    "            # Emotion Graph\n",
    "            plt.subplot(4, 1, 3)\n",
    "            x_values = np.arange(len(emotions))  # Use the length of the emotions list as x-axis values\n",
    "            plt.scatter(x_values, [emotion_labels.index(e) if e is not None else -1 for e in emotions],\n",
    "                        color='green', s=5)  # Plot emotions as dots\n",
    "            plt.ylim(-1, len(emotion_labels))  # Set y-axis limits\n",
    "            plt.xlim(0, video_length_seconds*6)  # Set x-axis limits\n",
    "            plt.yticks(range(len(emotion_labels)), emotion_labels)  # Set y-tick labels\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)  # Add gridlines\n",
    "            plt.ylabel('Emotion', fontsize=12)  # Add y-axis label\n",
    "\n",
    "            # Hand Graph\n",
    "            plt.subplot(4, 1, 4)\n",
    "            plt.plot(np.arange(frame_index), left_hand, color='red', linewidth=line_width, marker='o', markersize=1, label='Left hand movement')\n",
    "            plt.plot(np.arange(frame_index), right_hand, color='blue', linewidth=line_width, marker='o', markersize=1, label='Right hand movement')\n",
    "            plt.ylim([min(min_left_hand, min_right_hand), max(max_left_hand, max_right_hand)])\n",
    "            plt.xlim([0, video_length_seconds*6])\n",
    "            plt.axis('on')\n",
    "            plt.ylabel('Hand movements', fontsize=12)\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the graph with labels\n",
    "            plt.savefig('temp_graph.png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "            plt.clf()\n",
    "\n",
    "            graph = cv2.imread('temp_graph.png')\n",
    "\n",
    "            # Resize the graph to match the video width and increased height\n",
    "            graph = cv2.resize(graph, (output_width, 3 * graph_height))\n",
    "\n",
    "            # Display the input video\n",
    "            cv2.imshow('Input Video', frame)\n",
    "\n",
    "            # Display the graph with labels\n",
    "            cv2.imshow('Graph', graph)\n",
    "\n",
    "            # Write the frame with graph to the output video\n",
    "            out.write(frame)\n",
    "\n",
    "        # Break the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release video objects and close OpenCV windows\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
