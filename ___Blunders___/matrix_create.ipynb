{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from pytube import YouTube\n",
    "import os\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate speaker position relative to the center\n",
    "def calculate_speaker_position(frame, shoulder_midpoint_x):\n",
    "    frame_width = frame.shape[1]\n",
    "    speaker_position = shoulder_midpoint_x - frame_width / 2\n",
    "    return speaker_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = []\n",
    "emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "emotion_model = load_model('emotion_model.h5')\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def one_hot_encode(number, num_classes=7):\n",
    "    \"\"\"\n",
    "    One-hot encodes a number from 1 to num_classes.\n",
    "    \"\"\"\n",
    "    encoding = [0] * num_classes\n",
    "    encoding[number - 1] = 1\n",
    "    return encoding\n",
    "\n",
    "def get_emotion(frame):\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(frame_grey)\n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = faces[0]\n",
    "        cv2.rectangle(frame, pt1=(x, y), pt2=(x + w, y + h), color=(0, 0, 255), thickness=2)\n",
    "        roi_gray = frame_grey[y - 5:y + h + 5, x - 5:x + w + 5]\n",
    "        if not roi_gray.size == 0:\n",
    "            roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "            image_pixels = img_to_array(roi_gray)\n",
    "            image_pixels = np.expand_dims(image_pixels, axis=0)\n",
    "        else:\n",
    "            image_pixels = None\n",
    "            # matrxOfCurrentSecond.append(0)\n",
    "            return [0,0,0,0,0,0,0]\n",
    "        image_pixels /= 255\n",
    "        predictions = emotion_model.predict(image_pixels)\n",
    "        max_index = np.argmax(predictions[0])\n",
    "        detected_emotion = emotion_labels[max_index]\n",
    "        emotions.append(detected_emotion)\n",
    "        # matrxOfCurrentSecond.append(max_index + 1)\n",
    "        return one_hot_encode(max_index + 1)\n",
    "    else:\n",
    "        emotions.append(None)\n",
    "        # matrxOfCurrentSecond.append(0)\n",
    "        return [0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def get_head_turn_angle(results):\n",
    "    list_of_head_turn_angles = []\n",
    "    left_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "    right_eye = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "    nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "    if left_eye and right_eye and nose:\n",
    "        eye_line_vector = np.array([right_eye.x - left_eye.x, right_eye.y - left_eye.y])\n",
    "        eye_left_nose_vector = np.array([nose.x - left_eye.x, nose.y - left_eye.y])\n",
    "        eye_right_nose_vector = np.array([right_eye.x - nose.x, right_eye.y - nose.y])\n",
    "\n",
    "        dot_product_left = np.dot(eye_line_vector, eye_left_nose_vector)\n",
    "        eye_line_magnitude = np.linalg.norm(eye_line_vector)\n",
    "        eye_left_nose_magnitude = np.linalg.norm(eye_left_nose_vector)\n",
    "\n",
    "        dot_product_right = np.dot(eye_line_vector, eye_right_nose_vector)\n",
    "        eye_right_nose_magnitude = np.linalg.norm(eye_right_nose_vector)\n",
    "\n",
    "        cosine_angle_left = dot_product_left / (eye_line_magnitude * eye_left_nose_magnitude)\n",
    "        cosine_angle_right = dot_product_right / (eye_line_magnitude * eye_right_nose_magnitude)\n",
    "\n",
    "        head_turn_angle_left = np.arccos(cosine_angle_left) * (180 / np.pi)\n",
    "        head_turn_angle_right = np.arccos(cosine_angle_right) * (180 / np.pi)\n",
    "\n",
    "        list_of_head_turn_angles.append(head_turn_angle_left)\n",
    "        list_of_head_turn_angles.append(head_turn_angle_right)\n",
    "    else:\n",
    "        list_of_head_turn_angles.append(0)\n",
    "        list_of_head_turn_angles.append(0)\n",
    "    return list_of_head_turn_angles\n",
    "\n",
    "def get_shoulder_midpoint(results):\n",
    "    shoulder_midpoints = []\n",
    "    left_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "    right_shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "    if left_shoulder and right_shoulder:\n",
    "        shoulder_midpoint_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        shoulder_midpoints.append(shoulder_midpoint_x)\n",
    "    else:\n",
    "        shoulder_midpoints.append(0)\n",
    "    return shoulder_midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "\n",
    "excel_file = 'testdata10videos.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "output_folder = \"Transcripts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # video_id = row[\"youtube_video_code\"]\n",
    "    # if os.path.exists(f'matrix_of_video_{row[\"_id\"]}.npy'):\n",
    "    #     print(f\"Skipping video {video_id} because matrix already exists\")\n",
    "    #     continue\n",
    "    # matrixOfCurrentVideo = []\n",
    "\n",
    "    # if isinstance(video_id, str):\n",
    "    #     transcript = download_transcript_with_timestamps(video_id)\n",
    "    #     file_name = os.path.join(output_folder, f\"transcript_{row['_id']}.txt\")\n",
    "    #     with open(file_name, \"w\", encoding=\"utf-16\") as file:\n",
    "    #         file.write(transcript)\n",
    "    # else:\n",
    "    #     print(f\"Skipping invalid video code: {video_id}\")\n",
    "    #     continue\n",
    "\n",
    "    # popularity_labels.append(convert_to_float(row['likes']) / convert_to_float(row['views']))\n",
    "\n",
    "    video_link = row['youtube_video_code']\n",
    "    youtube_url = \"https://www.youtube.com/watch?v=\" + video_link\n",
    "    yt = YouTube(youtube_url)\n",
    "    video_stream = yt.streams.get_highest_resolution()\n",
    "    cap = cv2.VideoCapture(video_stream.url)\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_count = int(cap.get(7))\n",
    "    fps = int(cap.get(5))\n",
    "    video_length_seconds = frame_count / fps\n",
    "\n",
    "    # matrixOfNumberOfWps, transcript_embedding = calculate_avg_words_between_timestamps(file_name, video_length_seconds)\n",
    "\n",
    "\n",
    "#     output_video = 'output_video.mp4'\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter(output_video, fourcc, fps, (output_width, output_height))\n",
    "\n",
    "# video_length_seconds/64\n",
    "\n",
    "    # frame_index = 0\n",
    "    current_second = 0\n",
    "    print(math.floor((row[\"duration\"] - 10)/64))\n",
    "    for numWindows in range(math.floor((row[\"duration\"] - 10)/64)):\n",
    "        if numWindows == 0:\n",
    "            continue\n",
    "        isret = True\n",
    "        \n",
    "        \n",
    "        matrixOfCurrentWindow = []\n",
    "        for window64sec in range(64):\n",
    "            matrxOfCurrentSecond = []\n",
    "            num = 0\n",
    "            for onesec in range(fps):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    isret = False\n",
    "                    break\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                \n",
    "                if num<8 and onesec % (math.floor(fps / 8)) == 0:\n",
    "                    num = num+1\n",
    "                    #emotion\n",
    "                    emotions_one_hot_encoding = get_emotion(frame)\n",
    "                    for e in emotions_one_hot_encoding:\n",
    "                        matrxOfCurrentSecond.append(e)\n",
    "                    \n",
    "\n",
    "                    results = pose.process(frame_rgb)\n",
    "\n",
    "                    if results.pose_landmarks is not None:\n",
    "                        # print(\"pose landmarks\")\n",
    "                        # mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                        shoulder_midpoint = get_shoulder_midpoint(results)\n",
    "                        for p in shoulder_midpoint:\n",
    "                            matrxOfCurrentSecond.append(p)\n",
    "                    \n",
    "                        #head turn angle\n",
    "                        head_angles = get_head_turn_angle(results)\n",
    "                        for a in head_angles:\n",
    "                            matrxOfCurrentSecond.append(a)\n",
    "\n",
    "                        #pose Landmarks 33\n",
    "                        for point in results.pose_landmarks.landmark:\n",
    "                            if point.visibility > 0.5:\n",
    "                                matrxOfCurrentSecond.append(point.x)\n",
    "                                matrxOfCurrentSecond.append(point.y)\n",
    "                            else:\n",
    "                                matrxOfCurrentSecond.append(0)\n",
    "                                matrxOfCurrentSecond.append(0)\n",
    "\n",
    "                    else:\n",
    "                        matrxOfCurrentSecond.append(0)\n",
    "                        matrxOfCurrentSecond.append(0)\n",
    "                        matrxOfCurrentSecond.append(0)\n",
    "\n",
    "                        for i in range(33):\n",
    "                            matrxOfCurrentSecond.append(0)\n",
    "                            matrxOfCurrentSecond.append(0)\n",
    "                    \n",
    "               \n",
    "                    # cv2.imshow('Video Frame', frame)\n",
    "                    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    #     break\n",
    "                    # if num==8 and current_second<row[\"duration\"]:\n",
    "                    #     for a in transcript_embedding[current_second]:\n",
    "                    #         matrxOfCurrentSecond.append(a)\n",
    "                        \n",
    "            \n",
    "                if current_second>=row[\"duration\"] :\n",
    "                    break\n",
    "                # if len(matrxOfCurrentSecond ) < 1376 :\n",
    "                #     break\n",
    "\n",
    "            if current_second>=row[\"duration\"] :\n",
    "                break\n",
    "            if len(matrxOfCurrentSecond ) < 1376 : ########## 1376 to be changed.\n",
    "                break\n",
    "            current_second += 1\n",
    "            print(current_second)\n",
    "            matrixOfCurrentWindow.append(matrxOfCurrentSecond)\n",
    "            # print(len(matrxOfCurrentSecond))\n",
    "            \n",
    "            \n",
    "            \n",
    "            time.sleep(0.03)  \n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "            # if window64sec==63:\n",
    "        # matrixOfVideos.append(matrixOfCurrentWindow)\n",
    "        # popularity_labels.append(convert_to_float(row['likes']) / convert_to_float(row['views']))\n",
    "       \n",
    "        currentLabel = [] ########## to be added.....................\n",
    "        # os.makedirs(matrix_folder := '/dataSetFeatures/', exist_ok=True); np.save(os.path.join(matrix_folder, f'matrix_of_video_{row[\"_id\"]}_{numWindows}.npy'), matrixOfCurrentWindow)\n",
    "        # os.makedirs(labels_folder := '/labels/', exist_ok=True); np.save(os.path.join(labels_folder, f'popularity_labels_{row[\"_id\"]}_{numWindows}.npy'), currentLabel)\n",
    "\n",
    "        matrix_folder = 'TestDataSetFeatures'\n",
    "        labels_folder = 'TestRating'\n",
    "\n",
    "        # Make sure the folders exist, create them if not\n",
    "        os.makedirs(matrix_folder, exist_ok=True)\n",
    "        os.makedirs(labels_folder, exist_ok=True)\n",
    "\n",
    "        # Assuming row[\"_id\"] and numWindows are defined\n",
    "        filename_matrix = f'matrix_of_video_{row[\"_id\"]}_{numWindows}.npy'\n",
    "        filename_labels = f'popularity_labels_{row[\"_id\"]}_{numWindows}.npy'\n",
    "\n",
    "        # Full paths\n",
    "        full_path_matrix = os.path.join(matrix_folder, filename_matrix)\n",
    "        full_path_labels = os.path.join(labels_folder, filename_labels)\n",
    "\n",
    "        # Saving the NumPy arrays\n",
    "        np.save(full_path_matrix, matrixOfCurrentWindow)\n",
    "        np.save(full_path_labels, currentLabel)\n",
    "    \n",
    "    # matrixOfVideos.append(matrixOfCurrentVideo)\n",
    "    # np.save(f'matrix_of_video_{row[\"_id\"]}.npy', matrixOfCurrentVideo)\n",
    "    # print(len(matrixOfCurrentVideo))\n",
    "    cap.release()\n",
    "    # out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# matrix_array = np.array(matrixOfVideos)\n",
    "# # Save\n",
    "# np.save('matrix_of_videos.npy', matrix_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands to install ffmpeg in a separate terminal window:\n",
    "# sudo apt update \n",
    "# sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def download_audio(youtube_url, output_path):\n",
    "    try:\n",
    "        # Create a YouTube object\n",
    "        youtube = YouTube(youtube_url)\n",
    "\n",
    "        # Get the highest quality audio stream\n",
    "        audio_stream = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "        # Download the audio in its original format (usually webm or mp4)\n",
    "        temp_file = audio_stream.download(output_path)\n",
    "\n",
    "        # Define the output filename\n",
    "        output_file = os.path.join(output_path, \"audio.mp3\")\n",
    "\n",
    "        # Convert the downloaded file to mp3 using pydub\n",
    "        audio = AudioSegment.from_file(temp_file)\n",
    "        audio.export(output_file, format=\"mp3\")\n",
    "\n",
    "        # Optionally, remove the original downloaded file\n",
    "        os.remove(temp_file)\n",
    "\n",
    "        print(f\"Audio downloaded and converted successfully to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def calculate_engagement(audio_path):\n",
    "    # Load audio file\n",
    "    audio, _ = librosa.load(audio_path)\n",
    "\n",
    "    # Calculate energy (amplitude) over time\n",
    "    energy = np.abs(librosa.stft(audio))\n",
    "\n",
    "    # Calculate mean energy for each frame\n",
    "    mean_energy = np.mean(energy, axis=0)\n",
    "\n",
    "    # Calculate the standard deviation of energy to identify peaks\n",
    "    energy_std = np.std(mean_energy)\n",
    "\n",
    "    # Assess engagement based on energy\n",
    "    if energy_std > threshold_peak:\n",
    "        engagement_score = \"High (Alerting)\"\n",
    "    elif energy_std < threshold_low:\n",
    "        engagement_score = \"Low (Boring)\"\n",
    "    else:\n",
    "        engagement_score = \"Moderate\"\n",
    "\n",
    "    return engagement_score\n",
    "\n",
    "\n",
    "# Replace 'your_youtube_url' with the actual YouTube video URL\n",
    "video_url = 'https://www.youtube.com/watch?v=Tvn0E3W7I88'\n",
    "\n",
    "# Replace 'output_audio.mp3' with the desired output audio file name\n",
    "output_file = 'audio'\n",
    "download_audio(video_url, output_file)\n",
    "\n",
    "# Define thresholds for peak and low energy\n",
    "threshold_peak = 0.05  # Adjust as needed\n",
    "threshold_low = 0.01   # Adjust as needed\n",
    "\n",
    "# Example usage\n",
    "\n",
    "audio_path = \"/home/jyoti/Documents/GitHub/Articulation-Meter/audio/audio.mp3\"  # Replace with the actual path to your audio file\n",
    "engagement_score = calculate_engagement(audio_path)\n",
    "print(f\"Engagement Score: {engagement_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pytube import YouTube\n",
    "\n",
    "def download_audio(youtube_url, output_path):\n",
    "    try:\n",
    "        # Create a YouTube object\n",
    "        youtube = YouTube(youtube_url)\n",
    "\n",
    "        # Get the highest quality audio stream\n",
    "        audio_stream = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "        # Download the audio\n",
    "        temp_file = audio_stream.download(output_path)\n",
    "\n",
    "        # Define the output filename\n",
    "        output_file = os.path.join(output_path, \"audio.mp3\")\n",
    "\n",
    "        # Convert the downloaded file to mp3\n",
    "        audio = AudioSegment.from_file(temp_file)\n",
    "        audio.export(output_file, format=\"mp3\")\n",
    "\n",
    "        # Optionally, remove the original downloaded file\n",
    "        os.remove(temp_file)\n",
    "\n",
    "        print(f\"Audio downloaded and converted successfully to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_engagement(audio_path):\n",
    "    # Load audio file\n",
    "    audio, _ = librosa.load(audio_path)\n",
    "\n",
    "    # Calculate energy (amplitude) over time\n",
    "    energy = np.abs(librosa.stft(audio))\n",
    "\n",
    "    # Calculate mean energy for each frame\n",
    "    mean_energy = np.mean(energy, axis=0)\n",
    "\n",
    "    # Calculate the standard deviation of energy to identify peaks\n",
    "    energy_std = np.std(mean_energy)\n",
    "\n",
    "    # Assess engagement based on energy\n",
    "    if energy_std > threshold_peak:\n",
    "        engagement_score = \"High (Alerting)\"\n",
    "    elif energy_std < threshold_low:\n",
    "        engagement_score = \"Low (Boring)\"\n",
    "    else:\n",
    "        engagement_score = \"Moderate\"\n",
    "\n",
    "    return engagement_score\n",
    "\n",
    "\n",
    "# Replace 'your_youtube_url' with the actual YouTube video URL\n",
    "video_url = 'https://www.youtube.com/watch?v=Tvn0E3W7I88'\n",
    "\n",
    "# Replace 'output_audio.mp3' with the desired output audio file name\n",
    "output_file = 'audio'\n",
    "\n",
    "download_audio(video_url, output_file)\n",
    "\n",
    "# Define thresholds for peak and low energy\n",
    "threshold_peak = 0.05  # Adjust as needed\n",
    "threshold_low = 0.01   # Adjust as needed\n",
    "\n",
    "# Example usage\n",
    "audio_path = \"/home/asad/Documents/GitHub/Articulation-Meter/audio/audio.mp3\"  # Replace with the actual path to your audio file\n",
    "engagement_score = calculate_engagement(audio_path)\n",
    "print(f\"Engagement Score: {engagement_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install youtube-transcript-api\n",
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi  \n",
    "import speech_recognition as sr\n",
    "\n",
    "def get_transcript(video_id):\n",
    "    try:\n",
    "        # video_id = video_url.split(\"v=\")[1]\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_text = \"\"\n",
    "\n",
    "        for entry in transcript:\n",
    "            start = entry[\"start\"]\n",
    "            text = entry[\"text\"]\n",
    "            transcript_text += f\"{text}\\n\"\n",
    "\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "print(get_transcript(\"KLiXmteCvUI\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
